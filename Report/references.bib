@article{brockman2016openai,
  title={Openai gym},
  author={Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
  journal={arXiv preprint arXiv:1606.01540},
  year={2016}
}

@article{cheetah,
    title={Benchmarking Model-Based Reinforcement Learning},
    author={Tingwu Wang and Xuchan Bao and Ignasi Clavera and Jerrick Hoang and Yeming Wen and Eric Langlois and Shunshi Zhang and Guodong Zhang and Pieter Abbeel and Jimmy Ba},
    year={2019},
    url={https://arxiv.org/pdf/1907.02057.pdf}
}

@misc{deepmind,
    title={Deep Reinforcement Learning},
    author={DeepMind},
    year={2016},
    url={https://deepmind.google/discover/blog/deep-reinforcement-learning/}
}

@book{plaat-deeprl,
   title={Deep Reinforcement Learning},
   ISBN={9789811906381},
   url={http://dx.doi.org/10.1007/978-981-19-0638-1},
   DOI={10.1007/978-981-19-0638-1},
   publisher={Springer Nature Singapore},
   author={Plaat, Aske},
   year={2022} }

@book{sutton-barlo,
  title={The Bible of RL},
  author={Richard S. Sutton , Andrew G. Barto},
  year={2018},
  page={326}
}

@article{playigatari,
    title={Playing Atari With Deep Reinforcement Learning},
    author={Volodymyr Mnih and Koray Kavukcuoglu and David Silver and Alex Graves and Ioannis Antonoglou and Daan Wierstra and Martin Riedmiller},
    year={2018},
    url={https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf}
}


@article{ucb,
    title={Finite Time Analysis of Multi-armed Bandit Problem},
    author={Peter Auer and Nicolo Cesa-Bianchi and Paul Fischer},
    year={2002},
    url={https://rdcu.be/dCLjy}
}

@article{thompson,
    title={An empirical evaluation of Thompson Sampling},
    author={Olivier Chapelle and Lihong Li},
    year={2011},
    url={https://papers.nips.cc/paper_files/paper/2011/hash/e53a0a2978c28872a4505bdb51db06dc-Abstract.html}
}

@article{actor-critic,
    author = {Ivo Grondman and Lucian Busoniu and Gabriel Lopes and Robert Babuska},
    title = { A survey of actor-critic reinforcement learning: standard and natural policy gradients},
    journal = {IEEE Transactions on Systems, Man, and Cybernetics},
    year = {2012},
    url = {https://hal.science/hal-00756747/document}
}

@unpublished{david-ucl-lecture,
    author = {David Silver},
    title = {UCL Course on Reinforcement Learning},
    note = {Lecture Number 7},
    url = {https://www.davidsilver.uk/wp-content/uploads/2020/03/pg.pdf}
}

#Other references 
#https://medium.com/intro-to-artificial-intelligence/the-actor-critic-reinforcement-learning-algorithm-c8095a655c14
# https://medium.com/[add at ] thechrisyoon/deriving-policy-gradients-and-implementing-reinforce-f887949bd63
# https://towardsdatascience.com/understanding-actor-critic-methods-931b97b6df3f

Reference: Chapelle, O., & Li, L. (2011). An empirical evaluation of Thompson Sampling. In Advances in Neural Information Processing Systems (pp. 2249-2257).



