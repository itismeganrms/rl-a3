@article{brockman2016openai,
  title={OpenAI gym},
  author={Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
  journal={arXiv preprint arXiv:1606.01540},
  year={2016}
}% used

@book{plaat-deeprl,
   title={Deep Reinforcement Learning},
   ISBN={9789811906381},
   url={http://dx.doi.org/10.1007/978-981-19-0638-1},
   DOI={10.1007/978-981-19-0638-1},
   publisher={Springer Nature Singapore},
   author={Plaat, Aske},
   year={2022} 
} %used


@book{sutton-barlo,
  title= {Reinforcement Learning: An Introduction},
  author={Richard S. Sutton , Andrew G. Barto},
 publisher={The MIT Press},
  year={2018},
   url = {https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf}
}

@article{thompson,
    title={An empirical evaluation of Thompson Sampling},
    author={Olivier Chapelle and Lihong Li},
    year={2011},
    url={https://papers.nips.cc/paper_files/paper/2011/hash/e53a0a2978c28872a4505bdb51db06dc-Abstract.html}
}

@article{actor-critic,
    author = {Ivo Grondman and Lucian Busoniu and Gabriel Lopes and Robert Babuska},
    title = { A survey of actor-critic reinforcement learning: standard and natural policy gradients},
    journal = {IEEE Transactions on Systems, Man, and Cybernetics},
    year = {2012},
    url = {https://hal.science/hal-00756747/document}
}

@unpublished{david-ucl-lecture,
    author = {David Silver},
    title = {UCL Course on Reinforcement Learning},
    note = {Lecture Number 7},
    url = {https://www.davidsilver.uk/wp-content/uploads/2020/03/pg.pdf},
    year = {2020}
}


@book{ronaldwilliams,
    author = {Ronald J Williams},
    title = {Simple statistical gradient-following algorithms for connectionist reinforcement learning},
    url = {https://doi.org/10.1007/BF00992696},
    year = {1992}

}
#Other references 
#https://medium.com/intro-to-artificial-intelligence/the-actor-critic-reinforcement-learning-algorithm-c8095a655c14
# https://medium.com/[add at ] thechrisyoon/deriving-policy-gradients-and-implementing-reinforce-f887949bd63
# https://towardsdatascience.com/understanding-actor-critic-methods-931b97b6df3f

Reference: Chapelle, O., & Li, L. (2011). An empirical evaluation of Thompson Sampling. In Advances in Neural Information Processing Systems (pp. 2249-2257).
### Not used

@article{playigatari,
    title={Playing Atari With Deep Reinforcement Learning},
    author={Volodymyr Mnih and Koray Kavukcuoglu and David Silver and Alex Graves and Ioannis Antonoglou and Daan Wierstra and Martin Riedmiller},
    year={2018},
    url={https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf}
}


@article{ucb,
    title={Finite Time Analysis of Multi-armed Bandit Problem},
    author={Peter Auer and Nicolo Cesa-Bianchi and Paul Fischer},
    year={2002},
    url={https://rdcu.be/dCLjy}
}
