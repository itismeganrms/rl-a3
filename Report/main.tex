%%%%%%%% ICML 2021 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%
\documentclass{article}
\usepackage[]{algorithm2e}
\usepackage{algorithmic}
\usepackage{algorithm}
% Recommended, but optional, packages for hs and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{appendix}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{hyperref}
% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}
\usepackage{icml2021}
\usepackage{float}
\usepackage{enumitem}

\begin{document}
\twocolumn[
\icmltitle{Reinforcement Learning 2024
Assignment 3: Policy-based Reinforcement Learning}

\icmlsetsymbol{equal}{*}
\begin{icmlauthorlist}
\icmlauthor{Sherry Usman}{equal,to}
\icmlauthor{Qin Zhipei}{equal,to}
\icmlauthor{Megan Mirnalini Sundaram R}{equal, to}
\end{icmlauthorlist}

\icmlaffiliation{to}{Leiden University}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, Deep Q-Learning, Experience Replay, Target Network}

\vskip 0.3in
]

\begin{abstract}

\end{abstract}

\section {Introduction}

\section {Environment Selection}

\section{Algorithm Variation}
Policy based reinforcement learning is mostly used for continuous action spaces. In policy-based reinforcement learning, there is no value function which determines the next best possible action. Instead, it uses a parameterized policy. A value function may be used for learning the best policy, but not for selecting the action. The implemented policy is then improved, based on the data from each episode using policy gradient methods. \cite{sutton-barlo}
The outline of policy-based algorithms is using a parameterized policy function, which is denoted by $\pi_\Theta$. 
Here, $\theta$ refers to the parameters of the function and $\pi$ refers to the function itself. After defining the policy function, a path $\tau$ is chosen. If $\tau$ is good, then the parameters $\theta$ are increased towards the path $\tau$. Otherwise, it is decreased. In this way, it is maintained until the goal is reached.    \cite{plaat-deeprl}
The various algorithms that were implemented to realise this approach are
\subsection{REINFORCE}
REINFORCE is a commonly used Monte-Carlo Policy Gradient algorithm. 
\subsection{Actor - Critic}
Actor-critic 
\subsubsection{Bootstrapping}
\subsubsection{Baseline Subtraction}
\subsubsection{Bootstrapping and Baseline Subtraction}

\section{Goals Achieved}
\subsection{Effects of policy gradients on variance}
\subsection{Effects of bootstrapping on the policy gradients}
\subsection{Comparison of Performance}
\subsection{Effect of entropy regularization on performance}

\end{document}

