# -*- coding: utf-8 -*-
"""Bootstrapping.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iCQm-LagJ94swXPpvWrn21MZuUmz-DlJ
"""

!pip install wandb
!pip install swig
!pip install gym[all]
!pip install pygame

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.distributions import Categorical

import gym
from tqdm import tqdm
import numpy as np
import matplotlib.pyplot as plt
from google.colab import files

DEVICE = "cpu"

"""
    A neural network to predict the probability distribution of actions based on observations.
    Inherits from nn.Module.
"""
class ActionPredictor(nn.Module):
    def __init__(self, input_size, num_actions):
        super(ActionPredictor, self).__init__()
        self.fc1 = nn.Linear(input_size, 128)  # First fully connected layer
        self.fc2 = nn.Linear(128, num_actions)  # Second fully connected layer

    def forward(self, observations):
        hidden = F.relu(self.fc1(observations))
        probabilities = torch.nn.functional.softmax(self.fc2(hidden), dim=1)  # Convert logits to probabilities

        return probabilities

#Different layers
class ActionPredictor2(nn.Module):
    def __init__(self, input_size, num_actions):
        super(ActionPredictor2, self).__init__()
        self.fc1 = nn.Linear(input_size, 128)
        self.fc2 = nn.Linear(128, 128)
        self.fc3 = nn.Linear(128, num_actions)

    def forward(self, observations):
        hidden = F.relu(self.fc1(observations))
        probabilities = torch.nn.functional.softmax(self.fc3(hidden), dim=1)

        return probabilities

class ActionPredictor3(nn.Module):
    def __init__(self, input_size, num_actions):
        super(ActionPredictor3, self).__init__()
        self.fc1 = nn.Linear(input_size, 128)
        self.fc2 = nn.Linear(128, 128)
        self.fc3 = nn.Linear(128, 128)
        self.fc4 = nn.Linear(128, num_actions)

    def forward(self, observations):
        hidden = F.relu(self.fc1(observations))
        probabilities = torch.nn.functional.softmax(self.fc4(hidden), dim=1)

        return probabilities

#Different Number of neurons
class ActionPredictor4(nn.Module):
    def __init__(self, input_size, num_actions):
        super(ActionPredictor4, self).__init__()
        self.fc1 = nn.Linear(input_size, 64)
        self.fc2 = nn.Linear(64,num_actions)

    def forward(self, observations):
        hidden = F.relu(self.fc1(observations))
        probabilities = torch.nn.functional.softmax(self.fc2(hidden), dim=1)

        return probabilities

class ActionPredictor5(nn.Module):
    def __init__(self, input_size, num_actions):
        super(ActionPredictor5, self).__init__()
        self.fc1 = nn.Linear(input_size, 256)
        self.fc2 = nn.Linear(256, num_actions)

    def forward(self, observations):
        hidden = F.relu(self.fc1(observations))
        probabilities = torch.nn.functional.softmax(self.fc2(hidden), dim=1)

        return probabilities

class ValueEstimator(nn.Module):
    def __init__(self, num_features):
        super(ValueEstimator, self).__init__()
        self.dense1 = nn.Linear(num_features, 128)
        self.dense2 = nn.Linear(128, 1)

    def forward(self, state):
        intermediate = F.relu(self.dense1(state))
        estimated_value = self.dense2(intermediate)

        return estimated_value

class ValueEstimator2(nn.Module):
    def __init__(self, num_features):
        super(ValueEstimator2, self).__init__()
        self.dense1 = nn.Linear(num_features, 128)
        self.dense2 = nn.Linear(128, 128)
        self.dense3 = nn.Linear(128, 1)

    def forward(self, state):
        intermediate = F.relu(self.dense1(state))
        estimated_value = self.dense3(intermediate)

        return estimated_value

class ValueEstimator3(nn.Module):
    def __init__(self, num_features):
        super(ValueEstimator3, self).__init__()
        self.dense1 = nn.Linear(num_features, 128)
        self.dense2 = nn.Linear(128, 128)
        self.dense3 = nn.Linear(128, 128)
        self.dense4 = nn.Linear(128, 1)

    def forward(self, state):
        intermediate = F.relu(self.dense1(state))
        estimated_value = self.dense4(intermediate)

        return estimated_value


class ValueEstimator4(nn.Module):
    def __init__(self, num_features):
        super(ValueEstimator4, self).__init__()
        self.dense1 = nn.Linear(num_features, 64)
        self.dense2 = nn.Linear(64, 1)

    def forward(self, state):
        intermediate = F.relu(self.dense1(state))
        estimated_value = self.dense2(intermediate)

        return estimated_value


class ValueEstimator5(nn.Module):
    def __init__(self, num_features):
        super(ValueEstimator5, self).__init__()
        self.dense1 = nn.Linear(num_features, 256)
        self.dense2 = nn.Linear(256, 1)

    def forward(self, state):
        intermediate = F.relu(self.dense1(state))
        estimated_value = self.dense2(intermediate)

        return estimated_value

environment = gym.make('LunarLander-v2')
print(environment.observation_space.shape[0],environment.action_space.n)
action_net1 = ActionPredictor(environment.observation_space.shape[0], environment.action_space.n).to(DEVICE)
action_net2 = ActionPredictor2(environment.observation_space.shape[0], environment.action_space.n).to(DEVICE)
action_net3 = ActionPredictor3(environment.observation_space.shape[0], environment.action_space.n).to(DEVICE)
action_net4 = ActionPredictor4(environment.observation_space.shape[0], environment.action_space.n).to(DEVICE)
action_net5 = ActionPredictor5(environment.observation_space.shape[0], environment.action_space.n).to(DEVICE)
value_net1 = ValueEstimator(environment.observation_space.shape[0]).to(DEVICE)
value_net2 = ValueEstimator2(environment.observation_space.shape[0]).to(DEVICE)
value_net3 = ValueEstimator3(environment.observation_space.shape[0]).to(DEVICE)
value_net4 = ValueEstimator4(environment.observation_space.shape[0]).to(DEVICE)
value_net5 = ValueEstimator5(environment.observation_space.shape[0]).to(DEVICE)

'''learning rates'''
action_optimizer1_1_1 = optim.Adam(action_net1.parameters(), lr=0.005)
value_optimizer1_1_1 = optim.Adam(value_net1.parameters(), lr=0.005)
action_optimizer1_1_2 = optim.Adam(action_net1.parameters(), lr=0.05)
value_optimizer1_1_2 = optim.Adam(value_net1.parameters(), lr=0.05)
action_optimizer1_1_3 = optim.Adam(action_net1.parameters(), lr=0.001)
value_optimizer1_1_3 = optim.Adam(value_net1.parameters(), lr=0.001)

'''networks'''
action_optimizer1_2_1 = optim.Adam(action_net2.parameters(), lr=0.005)
action_optimizer1_3_1 = optim.Adam(action_net3.parameters(), lr=0.005)
action_optimizer1_4_1 = optim.Adam(action_net4.parameters(), lr=0.005)
action_optimizer1_5_1 = optim.Adam(action_net5.parameters(), lr=0.005)

value_optimizer1_2_1 = optim.Adam(value_net2.parameters(), lr=0.005)
value_optimizer1_3_1 = optim.Adam(value_net3.parameters(), lr=0.005)
value_optimizer1_4_1 = optim.Adam(value_net4.parameters(), lr=0.005)
value_optimizer1_5_1 = optim.Adam(value_net5.parameters(), lr=0.005)

def choose_action(action_net,environment_state):
    # Transform the state into a PyTorch tensor, add a batch dimension, and send to the computing device
    tensor_state = torch.tensor(environment_state, dtype=torch.float32).unsqueeze(0).to(DEVICE)

    # Obtain the action probabilities from the policy network using the processed state
    probabilities = action_net(tensor_state)
    tensor_state = tensor_state.detach()  # Optional: Detach state from graph if further graph-based operations are not required

    # Create a categorical distribution and sample an action
    action_distribution = Categorical(probabilities)
    sampled_action = action_distribution.sample().int()

    # Return the sampled action as a Python int and the log probability of that action
    return sampled_action.item(), action_distribution.log_prob(sampled_action)

# Setup tracking variables
episode_scores = []

def Run(action_net,value_net,action_optimizer,value_optimizer):
    episode_scores = []
    for ep in tqdm(range(Num_epo)):
        print(ep)
        # Initialize episode
        env_state = environment.reset()
        episode_done = False
        total_reward = 0
        Discount = 1

        # Episode execution
        for step in range(Max_steps):
            #print("step:{}".format(step))
            # Action selection and execution
            action_chosen, log_prob_action = choose_action(action_net,env_state)
            next_state, reward, episode_done, info = environment.step(action_chosen)
            #print(action_chosen,log_prob_action,reward)

            # Reward update
            total_reward += reward

            # Value calculations
            current_val = value_net(torch.from_numpy(env_state).float().unsqueeze(0).to(DEVICE))
            next_val = torch.tensor([0]).float().unsqueeze(0).to(DEVICE) if episode_done else value_net(torch.from_numpy(next_state).float().unsqueeze(0).to(DEVICE))

            # Loss calculations
            loss_value = F.mse_loss(reward + Gamma * next_val, current_val) * Discount
            loss_policy = -log_prob_action * (reward + Gamma * next_val - current_val) * Discount
            loss_policy = loss_policy.squeeze(1)

            # Optimize networks
            action_optimizer.zero_grad()
            #print("loss_policy:{},shape:{}".format(loss_policy,loss_policy.shape))
            loss_policy.backward(retain_graph=True)
            action_optimizer.step()

            value_optimizer.zero_grad()
            loss_value.backward()
            #print("loss_value:{},shape:{}".format(loss_value,loss_value.shape))
            value_optimizer.step()

            if episode_done:
               break

            # State transition
            env_state = next_state
            Discount *= Gamma

        # Record keeping
        episode_scores.append(total_reward)
        print("rewards:{}".format(total_reward))

    return episode_scores

Gamma = 0.98
Num_epo = 1000
Max_steps = 1000

#episode_scores1_1_1 = Run(action_net1,value_net1,action_optimizer1_1_1,value_optimizer1_1_1)

'''learning rates'''
#episode_scores1_1_2 = Run(action_net1,value_net1,action_optimizer1_1_2,value_optimizer1_1_2) #lr0.05
#episode_scores1_1_3 = Run(action_net1,value_net1,action_optimizer1_1_3,value_optimizer1_1_3) #lr0.001


'''hidden layers'''
#episode_scores1_2_1 = Run(action_net2,value_net2,action_optimizer1_2_1,value_optimizer1_2_1) #two layers
#episode_scores1_3_1 = Run(action_net3,value_net3,action_optimizer1_3_1,value_optimizer1_3_1) #three layers

'''neurons'''
episode_scores1_4_1 = Run(action_net4,value_net4,action_optimizer1_4_1,value_optimizer1_4_1) #64 neurons
episode_scores1_5_1 = Run(action_net5,value_net5,action_optimizer1_5_1,value_optimizer1_5_1) #256 neurons



plt.plot(episode_scores,label='lr=0.005')
plt.plot(episode_scores1_1_2,label='lr=0.05')
plt.plot(episode_scores1_1_3,label='lr=0.001')
plt.ylabel('score')
plt.xlabel('episodes')
plt.title('Training score of Lunar Lander Actor-Critic \n Different learning rates')
plt.legend()
plt.show()

plt.plot(episode_scores,label='one layer')
plt.plot(episode_scores1_2_1,label='two layers')
plt.plot(episode_scores1_3_1,label='three layers')
plt.ylabel('score')
plt.xlabel('episodes')
plt.title('Training score of Lunar Lander Actor-Critic \n Different hidden layers')
plt.legend()
plt.show()

plt.plot(episode_scores,label='128 neurons')
plt.plot(episode_scores1_4_1,label='64 neurons')
plt.plot(episode_scores1_5_1,label='256 neurons')
plt.ylabel('score')
plt.xlabel('episodes')
plt.title('Training score of Lunar Lander Actor-Critic \n Different number of neurons')
plt.legend()
plt.show()

from google.colab import files
with open("base.txt", "w") as file:
    for item in episode_scores:
        file.write(str(item) + "\n")

from google.colab import files
with open("lr=0.05.txt", "w") as file:
    for item in episode_scores1_1_2:
        file.write(str(item) + "\n")

from google.colab import files
with open("lr=0.001.txt", "w") as file:
    for item in episode_scores1_1_3:
        file.write(str(item) + "\n")

with open("two layers.txt", "w") as file:
    for item in episode_scores1_2_1:
        file.write(str(item) + "\n")

with open("three layers.txt", "w") as file:
    for item in episode_scores1_3_1:
        file.write(str(item) + "\n")

with open("64 neurons.txt", "w") as file:
    for item in episode_scores1_4_1:
        file.write(str(item) + "\n")

with open("256 neurons.txt", "w") as file:
    for item in episode_scores1_5_1:
        file.write(str(item) + "\n")

#filename = 'two layers.txt'
def load(filename):
    float_numbers = []
    with open(filename, 'r') as file:
          for line in file:
              float_num = float(line.strip())
              float_numbers.append(float_num)
    print(float_numbers)
    return float_numbers

episode_scores = load('base.txt')
episode_scores1_2_1 = load('two layers.txt')
episode_scores1_3_1 = load('three layers.txt')

