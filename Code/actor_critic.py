# -*- coding: utf-8 -*-
"""actor_critic.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HHrbeaqyDV4iffHAXP8rcHdmldDV14kd
"""

!pip install wandb
!pip install swig
!pip install gym[all]
!pip install pygame

import torch
import gym
import numpy as np
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import random
from torch.distributions import Categorical


class ActorNetwork(nn.Module):
      def __init__(self, n_input, n_output):
          super().__init__() #initialise module

          self.input_layer = nn.Linear(n_input, 256)
          self.output_layer = nn.Linear(256, n_output)

      def forward(self,x):
           x = self.input_layer(x) #forward propagation
           x = F.relu(x)
           action_probs = F.softmax(x, dim=1)
           return action_probs


class CriticNetwork(nn.Module):
      def __init__(self, n_input, n_output):
          super().__init__() #initialise module
          self.input_layer = nn.Linear(n_input, 256)
          self.output_layer = nn.Linear(256, n_output)


      def forward(self,x):
           x = self.input_layer(x) #forward propagation
           x = F.relu(x)

           state = self.output_layer(x)
           return state

def run(env, actor, critic, actorOptim, criticOptim, n_step, n_episode, discount_factor=0.99):
  state = env.reset()
  episode_array = [i for i in range(1, n_episode+1)]
  rewards_array = []
  I = 1


  for episode in range(n_episode):
    epi_reward = 0
    log_probs = []
    values = []
    rewards = []

    for step in range(n_step):

        action, lp = select_action(actor, state)
        action = action.item()

        next_state, reward, done, _ = env.step(action) #get next state, reward, done and info

        #s = s_ #update state
        epi_reward += reward

        state_tensor = torch.from_numpy(state).float().unsqueeze(0)
        state_val = critic(state_tensor)


        #get state value of next state
        next_state_tensor = torch.from_numpy(next_state).float().unsqueeze(0)
        next_state_val = critic(next_state_tensor)

        state_val_action = state_val.squeeze()[action]  # Select value corresponding to the action taken
        next_state_val_action = next_state_val.squeeze()[action]

        td_target = reward + discount_factor * next_state_val_action
        td_error = td_target - state_val_action


        #calculate value function loss with MSE
        critic_loss = F.mse_loss(state_val_action, td_target.detach())

        logits = actor(torch.from_numpy(state).float().unsqueeze(0))  # Assuming actor returns logits
        action_probs = F.softmax(logits, dim=-1)
        chosen_action_prob = action_probs[0, action]  # Select the probability of the chosen action
        actor_loss = -torch.log(chosen_action_prob) * td_error.detach().mean()

        #Backpropagate policy
        actorOptim.zero_grad()
        actor_loss.backward(retain_graph=True)
        actorOptim.step()

        #Backpropagate value
        criticOptim.zero_grad()
        critic_loss.backward()
        criticOptim.step()

        next_state = torch.FloatTensor(state).unsqueeze(0)

        if done:
          break


    rewards_array.append(epi_reward)

  return episode_array, rewards_array

def select_action(network, state):
  state = torch.from_numpy(state).unsqueeze(0)

  probs = network(state)
  state = state.detach()

  x = Categorical(probs)
  action = x.sample()
  #print("action here: {}".format(action))
  action = torch.argmax(action)
  #print("action here 2: {}".format(action))
  return action, x.log_prob(action)

def main():
  #n_episode = 10

  #Make environment
  env = gym.make('LunarLander-v2')
  action_space = env.action_space.n
  obs_space = env.observation_space.shape[0]


  #Initialise actor network and actor optimiser
  actor = ActorNetwork(obs_space, action_space)
  actorOptim = optim.Adam(actor.parameters(), lr=0.1)


  #Initialise policy network and policy optimiser
  critic = CriticNetwork(obs_space, action_space)
  criticOptim = optim.Adam(critic.parameters(), lr=0.1)

  n_step = 1000
  n_episode = 20
  n_output = env.action_space.n

  e_array, r_array = run(env, actor, critic, actorOptim, criticOptim, n_step, n_episode)
  env.close()
  print("e_array: {}".format(e_array))
  print("r_array:{}".format(r_array))




main()

!wandb login
import wandb
import random

# start a new wandb run to track this script
wandb.init(
    # set the wandb project where this run will be logged
    project="my-awesome-project",

    # track hyperparameters and run metadata
    config={
    "learning_rate": 0.02,
    "architecture": "CNN",
    "dataset": "CIFAR-100",
    "epochs": 10,
    }
)

# simulate training
epochs = 10
offset = random.random() / 5
for epoch in range(2, epochs):
    acc = 1 - 2 ** -epoch - random.random() / epoch - offset
    loss = 2 ** -epoch + random.random() / epoch + offset

    # log metrics to wandb
    wandb.log({"acc": acc, "loss": loss})

# [optional] finish the wandb run, necessary in notebooks
wandb.finish()