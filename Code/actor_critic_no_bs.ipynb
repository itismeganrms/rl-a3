{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb\n",
        "!pip install swig\n",
        "!pip install gym[all]\n",
        "!pip install pygame"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jwIgV-yZcldb",
        "outputId": "8b04f07e-4f1c-4e41-9474-cf7b6d29eac8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.16.6-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-2.0.1-py2.py3-none-any.whl (266 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.8/266.8 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.43 docker-pycreds-0.4.0 gitdb-4.0.11 sentry-sdk-2.0.1 setproctitle-1.3.3 smmap-5.0.1 wandb-0.16.6\n",
            "Collecting swig\n",
            "  Downloading swig-4.2.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: swig\n",
            "Successfully installed swig-4.2.1\n",
            "Requirement already satisfied: gym[all] in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym[all]) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym[all]) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym[all]) (0.0.8)\n",
            "Collecting mujoco==2.2.0 (from gym[all])\n",
            "  Downloading mujoco-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opencv-python>=3.0 in /usr/local/lib/python3.10/dist-packages (from gym[all]) (4.8.0.76)\n",
            "Collecting ale-py~=0.7.5 (from gym[all])\n",
            "  Downloading ale_py-0.7.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=3.0 in /usr/local/lib/python3.10/dist-packages (from gym[all]) (3.7.1)\n",
            "Requirement already satisfied: swig==4.* in /usr/local/lib/python3.10/dist-packages (from gym[all]) (4.2.1)\n",
            "Collecting pygame==2.1.0 (from gym[all])\n",
            "  Downloading pygame-2.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mujoco-py<2.2,>=2.1 (from gym[all])\n",
            "  Downloading mujoco_py-2.1.2.14-py3-none-any.whl (2.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m57.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting box2d-py==2.3.5 (from gym[all])\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pytest==7.0.1 (from gym[all])\n",
            "  Downloading pytest-7.0.1-py3-none-any.whl (296 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.0/297.0 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: imageio>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from gym[all]) (2.31.6)\n",
            "Collecting lz4>=3.1.0 (from gym[all])\n",
            "  Downloading lz4-4.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from mujoco==2.2.0->gym[all]) (1.4.0)\n",
            "Collecting glfw (from mujoco==2.2.0->gym[all])\n",
            "  Downloading glfw-2.7.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38-none-manylinux2014_x86_64.whl (211 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.8/211.8 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyopengl in /usr/local/lib/python3.10/dist-packages (from mujoco==2.2.0->gym[all]) (3.1.7)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from pytest==7.0.1->gym[all]) (23.2.0)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest==7.0.1->gym[all]) (2.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pytest==7.0.1->gym[all]) (24.0)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest==7.0.1->gym[all]) (1.5.0)\n",
            "Collecting py>=1.8.2 (from pytest==7.0.1->gym[all])\n",
            "  Downloading py-1.11.0-py2.py3-none-any.whl (98 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest==7.0.1->gym[all]) (2.0.1)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.7.5->gym[all]) (6.4.0)\n",
            "Requirement already satisfied: pillow<10.1.0,>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio>=2.14.1->gym[all]) (9.4.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->gym[all]) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->gym[all]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->gym[all]) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->gym[all]) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->gym[all]) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0->gym[all]) (2.8.2)\n",
            "Requirement already satisfied: Cython>=0.27.2 in /usr/local/lib/python3.10/dist-packages (from mujoco-py<2.2,>=2.1->gym[all]) (3.0.10)\n",
            "Requirement already satisfied: cffi>=1.10 in /usr/local/lib/python3.10/dist-packages (from mujoco-py<2.2,>=2.1->gym[all]) (1.16.0)\n",
            "Collecting fasteners~=0.15 (from mujoco-py<2.2,>=2.1->gym[all])\n",
            "  Downloading fasteners-0.19-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.10->mujoco-py<2.2,>=2.1->gym[all]) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0->gym[all]) (1.16.0)\n",
            "Building wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp310-cp310-linux_x86_64.whl size=2376102 sha256=8242020d17823c5e6e4ba59beb55be2ae357b55226342ad8e4536756d6b3dbe2\n",
            "  Stored in directory: /root/.cache/pip/wheels/db/8f/6a/eaaadf056fba10a98d986f6dce954e6201ba3126926fc5ad9e\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: glfw, box2d-py, pygame, py, mujoco, lz4, fasteners, ale-py, pytest, mujoco-py\n",
            "  Attempting uninstall: pygame\n",
            "    Found existing installation: pygame 2.5.2\n",
            "    Uninstalling pygame-2.5.2:\n",
            "      Successfully uninstalled pygame-2.5.2\n",
            "  Attempting uninstall: pytest\n",
            "    Found existing installation: pytest 7.4.4\n",
            "    Uninstalling pytest-7.4.4:\n",
            "      Successfully uninstalled pytest-7.4.4\n",
            "Successfully installed ale-py-0.7.5 box2d-py-2.3.5 fasteners-0.19 glfw-2.7.0 lz4-4.3.3 mujoco-2.2.0 mujoco-py-2.1.2.14 py-1.11.0 pygame-2.1.0 pytest-7.0.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "gym"
                ]
              },
              "id": "9796c62ffccd4b9b90b98dafb4e5f472"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pygame in /usr/local/lib/python3.10/dist-packages (2.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "Jkv7EYGedhWR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZ8KhFPmbeNg",
        "outputId": "a1277d62-d73f-432a-e65b-f17e96801939"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1, Total Reward: -162.72757433894577 Total Steps:97\n",
            "Episode 2, Total Reward: -386.8244617869609 Total Steps:107\n",
            "Episode 3, Total Reward: -282.2884240594841 Total Steps:124\n",
            "Episode 4, Total Reward: -221.1499931280496 Total Steps:74\n",
            "Episode 5, Total Reward: -53.467941647155556 Total Steps:88\n",
            "Episode 6, Total Reward: -57.601917405345205 Total Steps:80\n",
            "Episode 7, Total Reward: -282.695711214478 Total Steps:125\n",
            "Episode 8, Total Reward: -77.72296848166089 Total Steps:101\n",
            "Episode 9, Total Reward: -304.19966484819236 Total Steps:128\n",
            "Episode 10, Total Reward: -103.06458692845965 Total Steps:83\n",
            "Episode 11, Total Reward: -250.45611591347068 Total Steps:191\n",
            "Episode 12, Total Reward: -139.79158903591835 Total Steps:140\n",
            "Episode 13, Total Reward: -230.3864674647788 Total Steps:205\n",
            "Episode 14, Total Reward: -123.79491517913327 Total Steps:325\n",
            "Episode 15, Total Reward: -171.2049747949976 Total Steps:123\n",
            "Episode 16, Total Reward: -357.6563195912898 Total Steps:131\n",
            "Episode 17, Total Reward: -219.751969681506 Total Steps:93\n",
            "Episode 18, Total Reward: -252.56812325594746 Total Steps:112\n",
            "Episode 19, Total Reward: -184.37978311653174 Total Steps:307\n",
            "Episode 20, Total Reward: -105.62993177503041 Total Steps:92\n",
            "Episode 21, Total Reward: -296.35105348373645 Total Steps:201\n",
            "Episode 22, Total Reward: -276.78438253927027 Total Steps:219\n",
            "Episode 23, Total Reward: -149.14084285941576 Total Steps:140\n",
            "Episode 24, Total Reward: -218.44208411157106 Total Steps:136\n",
            "Episode 25, Total Reward: -203.57810524863166 Total Steps:225\n",
            "Episode 26, Total Reward: -172.5300821222551 Total Steps:203\n",
            "Episode 27, Total Reward: -170.2276534891293 Total Steps:150\n",
            "Episode 28, Total Reward: -163.9161289798252 Total Steps:225\n",
            "Episode 29, Total Reward: 97.02457607096049 Total Steps:1000\n",
            "Episode 30, Total Reward: -249.3050295981455 Total Steps:176\n",
            "Episode 31, Total Reward: -15.370162457329755 Total Steps:212\n",
            "Episode 32, Total Reward: -33.500088761315 Total Steps:180\n",
            "Episode 33, Total Reward: -51.721151368441085 Total Steps:171\n",
            "Episode 34, Total Reward: -205.36923331473696 Total Steps:222\n",
            "Episode 35, Total Reward: -294.63887152343875 Total Steps:342\n",
            "Episode 36, Total Reward: -254.7525118799064 Total Steps:304\n",
            "Episode 37, Total Reward: -248.75115451329498 Total Steps:403\n",
            "Episode 38, Total Reward: -207.7116115218848 Total Steps:228\n",
            "Episode 39, Total Reward: -44.57188424376838 Total Steps:344\n",
            "Episode 40, Total Reward: -39.80395432969536 Total Steps:177\n",
            "Episode 41, Total Reward: -256.8564088353278 Total Steps:213\n",
            "Episode 42, Total Reward: -216.55170694391694 Total Steps:363\n",
            "Episode 43, Total Reward: -73.9320571847829 Total Steps:384\n",
            "Episode 44, Total Reward: -252.35459764307876 Total Steps:327\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Actor Network\n",
        "class Actor(nn.Module):\n",
        "    def __init__(self, input_size=8, output_size=4, hidden_size=128):\n",
        "        super(Actor, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = self.relu(self.fc1(state))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Critic Network\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, input_size=8, hidden_size=128):\n",
        "        super(Critic, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = self.relu(self.fc1(state))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Actor-Critic Model\n",
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, actor, critic, lr):\n",
        "        super(ActorCritic, self).__init__()\n",
        "        self.actor = actor\n",
        "        self.critic = critic\n",
        "        self.actor_optimizer = optim.Adam(actor.parameters(), lr=0.001)\n",
        "        self.critic_optimizer = optim.Adam(critic.parameters(), lr=0.001)\n",
        "\n",
        "    def forward(self, state):\n",
        "        action_probs = torch.softmax(self.actor(state), dim=-1)\n",
        "        state_value = self.critic(state)\n",
        "        return action_probs, state_value\n",
        "\n",
        "\n",
        "\n",
        "# Training loop\n",
        "def update(agent, states, actions, rewards, next_states, dones, gamma=0.99):\n",
        "    states = torch.tensor(states, dtype=torch.float32)\n",
        "    actions = torch.tensor(actions, dtype=torch.int64).view(-1, 1)\n",
        "    rewards = torch.tensor(rewards, dtype=torch.float32).view(-1, 1)\n",
        "    next_states = torch.tensor(next_states, dtype=torch.float32)\n",
        "    dones = torch.tensor(dones, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "    # Compute TD targets\n",
        "    with torch.no_grad():\n",
        "        _, next_state_values = agent(next_states)\n",
        "        td_targets = rewards + gamma * (1 - dones) * next_state_values\n",
        "\n",
        "    # Compute advantages\n",
        "    _, state_values = agent(states)\n",
        "    advantages = td_targets - state_values\n",
        "\n",
        "    # Actor loss\n",
        "    action_probs, _ = agent(states)\n",
        "    log_probs = torch.log(action_probs.gather(1, actions))\n",
        "    actor_loss = -(log_probs * advantages.detach()).mean()\n",
        "\n",
        "    # Critic loss\n",
        "    critic_loss = nn.MSELoss()(state_values, td_targets)\n",
        "\n",
        "    # Update actor and critic networks\n",
        "    agent.actor_optimizer.zero_grad()\n",
        "    actor_loss.backward()\n",
        "    agent.actor_optimizer.step()\n",
        "\n",
        "    agent.critic_optimizer.zero_grad()\n",
        "    critic_loss.backward()\n",
        "    agent.critic_optimizer.step()\n",
        "\n",
        "def smoothing(array):\n",
        "  averages = []\n",
        "  for i in range(0, len(array), 5):\n",
        "      chunk = array[i:i+5]\n",
        "      avg = sum(chunk) / len(chunk)\n",
        "      averages.append(avg)\n",
        "  return averages\n",
        "\n",
        "def run(agent, env, n_episodes):\n",
        "  epi_reward = []\n",
        "  steps = []\n",
        "  for i_episode in range(n_episodes):\n",
        "    total_steps = 0\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "    while not done:\n",
        "        total_steps += 1\n",
        "        action_probs, _ = agent(torch.tensor(state, dtype=torch.float32).unsqueeze(0))\n",
        "        action = torch.multinomial(action_probs, num_samples=1).item()\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        total_reward += reward\n",
        "        update(agent, [state], [action], [reward], [next_state], [done])\n",
        "        state = next_state\n",
        "\n",
        "    print(f\"Episode {i_episode + 1}, Total Reward: {total_reward} Total Steps:{total_steps}\")\n",
        "    steps.append(total_steps)\n",
        "    epi_reward.append(total_reward)\n",
        "  env.close()\n",
        "  return epi_reward, steps\n",
        "\n",
        "def compute_averages(array):\n",
        "  averages = []\n",
        "  for i in range(0, len(array), 5):\n",
        "    temp = array[i:i+5]\n",
        "    avg = sum(temp) / len(temp)\n",
        "    averages.append(avg)\n",
        "  return averages\n",
        "\n",
        "def main():\n",
        "  # Create Lunar Lander environment\n",
        "  env = gym.make('LunarLander-v2')\n",
        "  state_size = env.observation_space.shape[0]\n",
        "  action_size = env.action_space.n\n",
        "  n_episodes = 1000\n",
        "\n",
        "  # Create actor, critic, and agent\n",
        "  actor1 = Actor(state_size, action_size)\n",
        "  critic1 = Critic(state_size)\n",
        "  agent1 = ActorCritic(actor1, critic1, 0.001)\n",
        "  e_array = [i for i in range(1, 201)]\n",
        "\n",
        "  r_array1, s_array1 = run(agent1, env, n_episodes)\n",
        "\n",
        "\n",
        "\n",
        "  r1 = smoothing(r_array1)\n",
        "  s1 = smoothing(s_array1)\n",
        "\n",
        "  # Define optimizer for actor and critic\n",
        "\n",
        "  actor2 = Actor()\n",
        "  critic2 = Critic()\n",
        "  agent2 = ActorCritic(actor2, critic2, 0.01)\n",
        "\n",
        "  r_array2, s_array2 = run(agent2, env, n_episodes)\n",
        "\n",
        "  r2 = smoothing(r_array2)\n",
        "  s2 = smoothing(s_array2)\n",
        "\n",
        "\n",
        "\n",
        "  actor3 = Actor()\n",
        "  critic3 = Critic()\n",
        "  agent3 = ActorCritic(actor3, critic3, 0.1)\n",
        "\n",
        "  r_array3, s_array3 = run(agent2, env, n_episodes)\n",
        "\n",
        "  r3 = compute_averages(r_array3)\n",
        "  s3 = compute_averages(s_array3)\n",
        "\n",
        "\n",
        "  plt.title(\"Performance of Actor-Critic with different learning rates\",fontsize=22)\n",
        "  plt.plot(e_array, r1, label = \"lr = 0.001\")\n",
        "  plt.plot(e_array, r2, label = \"lr = 0.01\")\n",
        "  plt.plot(e_array, r3, label = \"lr = 0.1\")\n",
        "  plt.xlabel('Episode')\n",
        "  plt.ylabel('Reward')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "  plt.title(\"Performance of Actor-Critic with different learning rates\",fontsize=22)\n",
        "  plt.plot(e_array, s1, label = \"lr = 0.001\")\n",
        "  plt.plot(e_array, s2, label = \"lr = 0.01\")\n",
        "  plt.plot(e_array, s3, label = \"lr = 0.1\")\n",
        "  plt.xlabel('Episode')\n",
        "  plt.ylabel('Steps')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "\n",
        "main()\n",
        "\n"
      ]
    }
  ]
}