# -*- coding: utf-8 -*-
"""REINFORCE_with_entropy_regularisation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rvvKc9OBwjTSDP9rWRLGFChhx4d1Tida
"""

!pip install wandb
!pip install swig
!pip install gym[all]
!pip install pygame

import gym
import numpy as np
import torch
import random
import math
import matplotlib.pyplot as plt
import torch.nn as nn
import torch.optim as optim
from torch.distributions import Categorical

# Define a simple policy network
class PolicyNetwork(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim, gamma=0.99, lr=0.001):
        super(PolicyNetwork, self).__init__()
        self.lr = lr
        self.gamma = gamma
        self.l1 = nn.Linear(input_dim, hidden_dim)
        self.l2 = nn.Linear(hidden_dim, output_dim)
        self.optimizer = optim.Adam(self.parameters(), lr=self.lr)

    def forward(self, x):
        x = torch.relu(self.l1(x))
        x = self.l2(x)
        return torch.softmax(x, dim=-1)

# Function to collect trajectories
def collect_trajectories(policy_network, env):
    trajectories = []
    total_reward = 0
    timesteps = 0
    episode = {'states': [], 'actions': [], 'rewards': []}
    state = env.reset()
    done = False
    while not done:
        timesteps += 1
        state = torch.tensor(state, dtype=torch.float32)
        action_probs = policy_network(state)
        action_dist = Categorical(action_probs)
        action = action_dist.sample()
        next_state, reward, done, _ = env.step(action.item())
        total_reward += reward
        episode['states'].append(state)
        episode['actions'].append(action)
        episode['rewards'].append(reward)
        state = next_state

    trajectories.append(episode)
    return trajectories, total_reward, timesteps

# Function to compute discounted rewards
def compute_discounted_rewards(rewards, gamma):
    discounted_rewards = []
    running_add = 0
    for r in reversed(rewards):
        running_add = running_add * gamma + r
        discounted_rewards.insert(0, running_add)
    return discounted_rewards

# Function to update the policy network with entropy regularization
def update_with_entropy_regularization(policy_network, trajectories, gamma, entropy_weight):
    for episode in trajectories:
        loss = 0
        entropy = 0
        discounted_rewards = compute_discounted_rewards(episode['rewards'], gamma)
        baseline = np.mean(discounted_rewards)
        for state, action, dr in zip(episode['states'], episode['actions'], discounted_rewards):
            action_probs = policy_network(state)
            action_dist = Categorical(action_probs)
            log_prob = action_dist.log_prob(action)
            entropy += action_dist.entropy()
            advantage = dr - baseline
            loss -= (log_prob * advantage) + (entropy_weight * entropy)

        policy_network.optimizer.zero_grad()
        loss.backward()
        policy_network.optimizer.step()

# Main function for training
def train():
    # Set up environment
    random.seed(350)
    env = gym.make('LunarLander-v2')
    input_dim = env.observation_space.shape[0]
    output_dim = env.action_space.n
    hidden_dim = 256

    # Hyperparameters
    n_episodes = 1000
    gamma = 0.99
    entropy_weight = 0.01  # Adjust this parameter to control the strength of entropy regularization

    policy_network = PolicyNetwork(input_dim, hidden_dim, output_dim, lr=0.001)
    rewards_array = []
    timesteps_array = []

    # Training loop
    for episode in range(n_episodes):
        trajectories, rewards, timesteps = collect_trajectories(policy_network, env)
        update_with_entropy_regularization(policy_network, trajectories, gamma, entropy_weight)
        rewards_array.append(rewards)
        timesteps_array.append(timesteps)
        print("Episode {}: Total Reward: {:.2f} Timesteps: {}".format(episode + 1, rewards, timesteps))

    policy_network_2 = PolicyNetwork(input_dim, hidden_dim, output_dim, lr=0.01)
    rewards_array2 = []
    timesteps_array2 = []

    # Training loop
    for episode in range(n_episodes):
        trajectories, rewards, timesteps = collect_trajectories(policy_network_2, env)
        update_with_entropy_regularization(policy_network_2, trajectories, gamma, entropy_weight)
        rewards_array2.append(rewards)
        timesteps_array2.append(timesteps)
        print("Episode {}: Total Reward: {:.2f} Timesteps: {}".format(episode + 1, rewards, timesteps))

    policy_network_3 = PolicyNetwork(input_dim, hidden_dim, output_dim, lr=0.1)
    rewards_array3 = []
    timesteps_array3 = []

    # Training loop
    for episode in range(n_episodes):
        trajectories, rewards, timesteps = collect_trajectories(policy_network_3, env)
        update_with_entropy_regularization(policy_network_3, trajectories, gamma, entropy_weight)
        rewards_array3.append(rewards)
        timesteps_array3.append(timesteps)
        print("Episode {}: Total Reward: {:.2f} Timesteps: {}".format(episode + 1, rewards, timesteps))



    # Plotting
    plt.title("Performance of REINFORCE (in rewards) with Entropy Regularization in Lunar Lander")
    plt.plot(range(1, n_episodes + 1), rewards_array, label="lr=0.001")
    plt.plot(range(1, n_episodes + 1), rewards_array2, label="lr=0.01")
    plt.plot(range(1, n_episodes + 1), rewards_array3, label="lr=0.1")
    plt.xlabel("Episodes")
    plt.ylabel("Total Rewards")
    plt.legend()
    plt.show()

    plt.title("Performance of REINFORCE (in steps) with Entropy Regularization in Lunar Lander")
    plt.plot(range(1, n_episodes + 1), timesteps_array, label="lr=0.001")
    plt.plot(range(1, n_episodes + 1), timesteps_array2, label="lr=0.01")
    plt.plot(range(1, n_episodes + 1), timesteps_array3, label="lr=0.1")
    plt.xlabel("Episodes")
    plt.ylabel("Timesteps")
    plt.legend()
    plt.show()

    env.close()

# Run the training
train()