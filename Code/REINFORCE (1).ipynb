{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb --quiet\n",
        "!pip install swig --quiet\n",
        "!pip install gym[all] --quiet\n",
        "!pip install pygame --quiet"
      ],
      "metadata": {
        "id": "bTN2O3ujWQKP",
        "outputId": "b7b94cb1-c0e8-47cb-f6e4-a5fd53ade52f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.8/266.8 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m63.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m59.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.0/297.0 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.8/211.8 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import random\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical"
      ],
      "metadata": {
        "id": "uvfBaIKoS1uW"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a simple policy network\n",
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, gamma=0.99, lr=0.001):\n",
        "        super(PolicyNetwork, self).__init__()\n",
        "        self.lr = lr\n",
        "        self.gamma = gamma\n",
        "        self.l1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.l2 = nn.Linear(hidden_dim, output_dim)\n",
        "        self.optimizer = optim.Adam(self.parameters(),lr=self.lr)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.l1(x))\n",
        "        x = self.l2(x)\n",
        "        return torch.softmax(x, dim=-1)"
      ],
      "metadata": {
        "id": "2p8HpCupS5zk",
        "outputId": "0d9eab43-8af3-4a0d-caad-996e10ce9b38",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to collect trajectories\n",
        "def collect_trajectories(policy_network, env):\n",
        "    trajectories = []\n",
        "    total_reward = 0\n",
        "    timesteps = 0\n",
        "    episode = {'states': [], 'actions': [], 'rewards': []}\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "      timesteps += 1\n",
        "      state = torch.tensor(state, dtype=torch.float32)\n",
        "      action_probs = policy_network(state)\n",
        "      action_dist = Categorical(action_probs)\n",
        "      action = action_dist.sample()\n",
        "      next_state, reward, done, _ = env.step(action.item())\n",
        "      total_reward += reward\n",
        "      timesteps += 1\n",
        "      episode['states'].append(state)\n",
        "      episode['actions'].append(action)\n",
        "      episode['rewards'].append(reward)\n",
        "      state = next_state\n",
        "    #print(\"Reward: {}\".format(total_reward))\n",
        "\n",
        "    trajectories.append(episode)\n",
        "    return trajectories, total_reward, timesteps  #returns total reward and timesteps per episode"
      ],
      "metadata": {
        "id": "_gtBimQbS7AF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to compute discounted rewards\n",
        "def compute_discounted_rewards(rewards, gamma):\n",
        "    discounted_rewards = []\n",
        "    running_add = 0\n",
        "    for r in reversed(rewards):\n",
        "        running_add = running_add * gamma + r\n",
        "        discounted_rewards.insert(0, running_add)\n",
        "    return discounted_rewards"
      ],
      "metadata": {
        "id": "PCNLoS3gS9gr"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to update the policy network\n",
        "def update_with_baseline(policy_network, trajectories, gamma):\n",
        "    for episode in trajectories:\n",
        "        loss = 0\n",
        "        discounted_rewards = compute_discounted_rewards(episode['rewards'], gamma)\n",
        "        baseline = np.mean(discounted_rewards)\n",
        "        for state, action, dr in zip(episode['states'], episode['actions'], discounted_rewards):\n",
        "            action_probs = policy_network(state)\n",
        "            action_dist = Categorical(action_probs)\n",
        "            log_prob = action_dist.log_prob(action)\n",
        "            advantage = dr - baseline\n",
        "            loss -= log_prob * advantage\n",
        "\n",
        "            #loss -= log_prob * dr\n",
        "        #total_loss = loss / len(trajectories)\n",
        "        policy_network.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        policy_network.optimizer.step()"
      ],
      "metadata": {
        "id": "LjoiF0GlTATp"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_without_baseline(policy_network, trajectories, gamma):\n",
        "    for episode in trajectories:\n",
        "        loss = 0\n",
        "        discounted_rewards = compute_discounted_rewards(episode['rewards'], gamma)\n",
        "        #baseline = np.mean(discounted_rewards)\n",
        "        for state, action, dr in zip(episode['states'], episode['actions'], discounted_rewards):\n",
        "            action_probs = policy_network(state)\n",
        "            action_dist = Categorical(action_probs)\n",
        "            log_prob = action_dist.log_prob(action)\n",
        "            #advantage = dr - baseline\n",
        "            loss -= log_prob * dr\n",
        "\n",
        "            #loss -= log_prob * dr\n",
        "        #total_loss = loss / len(trajectories)\n",
        "        policy_network.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        policy_network.optimizer.step()"
      ],
      "metadata": {
        "id": "FMy397iOTBvH"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main function for training\n",
        "def train():\n",
        "    # Set up environment\n",
        "    random.seed(350)\n",
        "    env = gym.make('LunarLander-v2')\n",
        "    input_dim = env.observation_space.shape[0]\n",
        "    output_dim = env.action_space.n\n",
        "    hidden_dim = 256\n",
        "    # Hyperparameters\n",
        "    n_episodes = 100\n",
        "    episode_array = [i for i in range(1,n_episodes+1)]\n",
        "    gamma = 0.99\n",
        "    target_update = 10\n",
        "\n",
        "    policy_network_1 = PolicyNetwork(input_dim, hidden_dim, output_dim, lr=0.001)\n",
        "    rewards_array_1 = []\n",
        "    timesteps_1 = []\n",
        "    mean_rewards_list_1 = []\n",
        "    #batch_size = 20\n",
        "    #print(\"{}\".format(math.floor(n_episodes/batch_size)))\n",
        "     # Training loop\n",
        "    for episode in range(n_episodes):\n",
        "        #print(\"episode: {}\".format((episode+1) * (n_episodes/batch_size) ))\n",
        "        trajectories, rewards, timesteps = collect_trajectories(policy_network_1, env)\n",
        "        update_without_baseline(policy_network_1, trajectories, gamma)\n",
        "        timesteps_1.append(timesteps)\n",
        "        rewards_array_1.append(rewards)\n",
        "        #trajectories_1.extend(trajectories)\n",
        "\n",
        "        #print(\"Episode {}: Total Reward: {:.2f} Timesteps: {}\".format(episode + 1, rewards, timesteps))\n",
        "        if episode % target_update == 0:\n",
        "            reward = np.mean(rewards_array_1[episode-10:episode])\n",
        "            print(\"Episode:{}\".format(episode))\n",
        "            print(\"Mean Reward:{}\".format(reward))\n",
        "            episode_array.append(episode)\n",
        "            mean_rewards_list_1.append(reward)\n",
        "\n",
        "\n",
        "    policy_network_2 = PolicyNetwork(input_dim, hidden_dim, output_dim, lr=0.01)\n",
        "    rewards_array_2 = []\n",
        "    timesteps_2 = []\n",
        "    mean_rewards_list_2 = []\n",
        "    for episode in range(n_episodes):\n",
        "        #print(\"episode: {}\".format((episode+1) * (n_episodes/batch_size) ))\n",
        "        trajectories, rewards, timesteps = collect_trajectories(policy_network_2, env)\n",
        "        update_without_baseline(policy_network_2, trajectories, gamma)\n",
        "        timesteps_2.append(timesteps)\n",
        "        rewards_array_2.append(rewards)\n",
        "        #trajectories_1.extend(trajectories)\n",
        "\n",
        "       # print(\"Episode {}: Total Reward: {:.2f} Timesteps: {}\".format(episode + 1, rewards, timesteps))\n",
        "        if episode % target_update == 0:\n",
        "            reward = np.mean(rewards_array_2[episode-10:episode])\n",
        "            print(\"Episode:{}\".format(episode))\n",
        "            print(\"Mean Reward:{}\".format(reward))\n",
        "            episode_array.append(episode)\n",
        "            mean_rewards_list_2.append(reward)\n",
        "\n",
        "\n",
        "    policy_network_3 = PolicyNetwork(input_dim,hidden_dim, output_dim, lr=0.1)\n",
        "    rewards_array_3 = []\n",
        "    timesteps_3 = []\n",
        "    mean_rewards_list_3 = []\n",
        "    for episode in range(n_episodes):\n",
        "        #print(\"episode: {}\".format((episode+1) * (n_episodes/batch_size) ))\n",
        "        trajectories, rewards, timesteps = collect_trajectories(policy_network_3, env)\n",
        "        update_without_baseline(policy_network_3, trajectories, gamma)\n",
        "        timesteps_3.append(timesteps)\n",
        "        rewards_array_3.append(rewards)\n",
        "        #trajectories_1.extend(trajectories)\n",
        "\n",
        "        #print(\"Episode {}: Total Reward: {:.2f} Timesteps: {}\".format(episode + 1, rewards, timesteps))\n",
        "        if episode % target_update == 0:\n",
        "            reward = np.mean(rewards_array_3[episode-10:episode])\n",
        "            print(\"Episode:{}\".format(episode))\n",
        "            print(\"Mean Reward:{}\".format(reward))\n",
        "            episode_array.append(episode)\n",
        "            mean_rewards_list_3.append(reward)\n",
        "\n",
        "    plt.title(\"Performance of REINFORCE in Lunar Lander\")\n",
        "    plt.plot(episode_array, rewards_array_1, label = \"lr = 0.001\")\n",
        "    plt.plot(episode_array, rewards_array_2, label = \"lr = 0.01\")\n",
        "    plt.plot(episode_array, rewards_array_3, label = \"lr = 0.1\")\n",
        "    plt.xlabel(\"Episodes\")\n",
        "    plt.ylabel(\"Rewards\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    plt.title(\"Performance of REINFORCE in Lunar Lander\")\n",
        "    plt.plot(episode_array, mean_rewards_list_1, label = \"lr = 0.001\")\n",
        "    plt.plot(episode_array, mean_rewards_list_2, label = \"lr = 0.01\")\n",
        "    plt.plot(episode_array, mean_rewards_list_3, label = \"lr = 0.1\")\n",
        "    plt.xlabel(\"Episodes\")\n",
        "    plt.ylabel(\"Rewards\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    plt.title(\"Performance of REINFORCE in Lunar Lander\")\n",
        "    plt.plot(episode_array, timesteps_1, label = \"lr = 0.001\")\n",
        "    plt.plot(episode_array, timesteps_2, label = \"lr = 0.01\")\n",
        "    plt.plot(episode_array, timesteps_3, label = \"lr = 0.1\")\n",
        "    plt.xlabel(\"Episodes\")\n",
        "    plt.ylabel(\"Timesteps\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    env.close()\n"
      ],
      "metadata": {
        "id": "zi27KnKxTD2e"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pxaJB_BWP97S",
        "outputId": "fcaf31d7-20c9-41d3-f2fa-04781efb226c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode:0\n",
            "Mean Reward:nan\n",
            "Episode:10\n",
            "Mean Reward:-159.95525953073079\n",
            "Episode:20\n",
            "Mean Reward:-218.13206001410973\n",
            "Episode:30\n",
            "Mean Reward:-358.7469252359992\n",
            "Episode:40\n",
            "Mean Reward:-243.66491695494386\n",
            "Episode:50\n",
            "Mean Reward:-182.347218782095\n",
            "Episode:60\n",
            "Mean Reward:-200.27870338240345\n",
            "Episode:70\n",
            "Mean Reward:-190.0321472062423\n",
            "Episode:80\n",
            "Mean Reward:-179.20830677351688\n",
            "Episode:90\n",
            "Mean Reward:-168.5781377469075\n",
            "Episode:0\n",
            "Mean Reward:nan\n",
            "Episode:10\n",
            "Mean Reward:-143.1061554482531\n",
            "Episode:20\n",
            "Mean Reward:-124.836122586373\n",
            "Episode:30\n",
            "Mean Reward:-129.98198267027718\n",
            "Episode:40\n",
            "Mean Reward:-134.11480866225182\n",
            "Episode:50\n",
            "Mean Reward:-147.21381301875198\n",
            "Episode:60\n",
            "Mean Reward:-154.97149169179528\n",
            "Episode:70\n",
            "Mean Reward:-124.86547624921081\n",
            "Episode:80\n",
            "Mean Reward:-130.10266012978485\n",
            "Episode:90\n",
            "Mean Reward:-113.25067314988264\n",
            "Episode:0\n",
            "Mean Reward:nan\n",
            "Episode:10\n",
            "Mean Reward:-427.8786357067446\n",
            "Episode:20\n",
            "Mean Reward:-816.3327128165122\n",
            "Episode:30\n",
            "Mean Reward:-1030.4476824603307\n",
            "Episode:40\n",
            "Mean Reward:-1148.495357824834\n",
            "Episode:50\n",
            "Mean Reward:-634.710871898488\n",
            "Episode:60\n",
            "Mean Reward:-794.4333950478797\n",
            "Episode:70\n",
            "Mean Reward:-991.286080826611\n",
            "Episode:80\n",
            "Mean Reward:-623.9375373617861\n",
            "Episode:90\n",
            "Mean Reward:-1072.4505223732856\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "x and y must have same first dimension, but have shapes (130,) and (100,)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-afe2c43fe7fa>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Run the training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-14-14c8137bb6fc>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Performance of REINFORCE in Lunar Lander\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards_array_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"lr = 0.001\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards_array_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"lr = 0.01\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards_array_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"lr = 0.1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2810\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0m_copy_docstring_and_deprecators\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2811\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2812\u001b[0;31m     return gca().plot(\n\u001b[0m\u001b[1;32m   2813\u001b[0m         \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscalex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscaley\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2814\u001b[0m         **({\"data\": data} if data is not None else {}), **kwargs)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1686\u001b[0m         \"\"\"\n\u001b[1;32m   1687\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1688\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1689\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1690\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m             yield from self._plot_args(\n\u001b[0m\u001b[1;32m    312\u001b[0m                 this, kwargs, ambiguous_fmt_datakey=ambiguous_fmt_datakey)\n\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m             raise ValueError(f\"x and y must have same first dimension, but \"\n\u001b[0m\u001b[1;32m    505\u001b[0m                              f\"have shapes {x.shape} and {y.shape}\")\n\u001b[1;32m    506\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (130,) and (100,)"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Run the training\n",
        "train()"
      ]
    }
  ]
}