# -*- coding: utf-8 -*-
"""Comparison_of_LR_Reinforce.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/itismeganrms/rl-a3/blob/main/Code/Comparison_of_LR_Reinforce.ipynb
"""

!pip install wandb --quiet
!pip install swig --quiet
!pip install gym[all] --quiet
!pip install pygame --quiet

import warnings
warnings.filterwarnings("ignore")

## fixing the pygame error

import os
os.environ['SDL_VIDEODRIVER']='dummy'
import pygame
pygame.display.set_mode((640,480))

import gym
#from gym.envs import box2d
import torch
from torch.distributions import Categorical
import statistics
import numpy as np
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt

class Policy_Network(nn.Module):
    def __init__(self, input_size, output_size, hidden_size = 128, gamma = 0.99, learning_rate = 0.01):
        super().__init__()
        self.input_size = input_size
        self.output_size = output_size
        self.hidden_size = hidden_size
        self.gamma = gamma
        self.learning_rate = learning_rate
        self.layer1 = nn.Linear(input_size, hidden_size)
        self.layer2 = nn.Linear(hidden_size, output_size)
        self.relu = nn.ReLU()
        self.softmax = nn.Softmax(dim=-1)

        self.optimizer = optim.Adam(self.parameters(),lr=self.learning_rate)


    def forward(self,x):
      x = self.layer1(x) #forward propagation
      x = self.layer2(x)
      x = self.softmax(x)
      return x

    def act(self,state):
        action_probs = torch.softmax(self.policy_network(state), dim=-1)
        return action_probs

def get_discounted_rewards(rewards, gamma):
        discounted_rewards = []
        #print("rewards: {}".format(rewards))
        sum = 0
        for r in reversed(rewards):
            sum = sum * gamma + r
            #pw = pw + 1
            discounted_rewards.insert(0, sum)
        return discounted_rewards

def update(policy_network, trajectories, gamma): #a trajectory for each episode
  loss = 0
  for t in trajectories:
    states = torch.tensor(t['states'], dtype=torch.float32)
    actions = torch.tensor(t['actions'], dtype=torch.int64)  # Assuming actions are integers
    action_probs = policy_network(states)
    action_dist = Categorical(action_probs)

    discounted_rewards = get_discounted_rewards(t['rewards'], gamma)

    for i in range(len(states)):
      action_prob = action_probs[i][actions[i]]  # Select the action probability for the chosen action
      log_prob = torch.log(action_prob)
      dr = discounted_rewards[i]
      loss = -log_prob * dr
      loss.requires_grad = True
      policy_network.optimizer.zero_grad()
      loss.backward()
      policy_network.optimizer.step()

def run(policy_network, env):
    episodic_reward = []
    dones = []
    episode = {"states": [], "actions": [], "rewards": []}
    trajectories = []
    state = env.reset()
    done = False
    log_probs_array = []
    total_reward = 0
    while not done:
        stateT = torch.tensor(state, dtype=torch.float32)
        action_probs = policy_network(stateT)
        action_probs.requires_grad = True
        action_dist = Categorical(action_probs)
        action = action_dist.sample()
        next_state, reward, done, _ = env.step(action.item())
        episode["states"].append(state)
        episode["actions"].append(action)
        episode["rewards"].append(reward)

        total_reward += reward
        state = next_state
    trajectories.append(episode)
    return total_reward, trajectories

def main():
    env = gym.make("LunarLander-v2")
    state_size = env.observation_space.shape[0]
    action_size = env.action_space.n


    n_episodes = 500
    gamma = 0.99
    episode_array = [i for i in range(1,n_episodes+1)]




    policy_network_1 = Policy_Network(state_size, action_size, learning_rate =0.001) #learning rate 0.001
    reward_array_1 = []

    for i in range(n_episodes):
      total_reward, trajectories = run(policy_network_1, env)
      update(policy_network_1,trajectories, gamma)
      reward_array_1.append(total_reward)
      print("Episode {}: Total Reward: {:.2f}".format(i + 1, total_reward))


    policy_network_2 = Policy_Network(state_size, action_size, learning_rate =0.01) #learning rate 0.001
    reward_array_2 = []

    for i in range(n_episodes):
      total_reward, trajectories = run(policy_network_2, env)
      update(policy_network_2,trajectories, gamma)
      reward_array_2.append(total_reward)
      print("Episode {}: Total Reward: {:.2f}".format(i + 1, total_reward))

    policy_network_3 = Policy_Network(state_size, action_size, learning_rate =0.1) #learning rate 0.001
    reward_array_3 = []

    for i in range(n_episodes):
      total_reward, trajectories = run(policy_network_3, env)
      update(policy_network_3,trajectories, gamma)
      reward_array_3.append(total_reward)
      print("Episode {}: Total Reward: {:.2f}".format(i + 1, total_reward))





    plt.title("Performance of REINFORCE in Lunar Lander")
    plt.plot(episode_array, reward_array_1, label = "lr = 0.001")
    plt.plot(episode_array,reward_array_2, label = "lr = 0.01")
    plt.plot(episode_array,reward_array_3, label = "lr = 0.1")
    plt.xlabel("Episodes")
    plt.ylabel("Rewards")
    plt.legend()
    plt.show()

main()