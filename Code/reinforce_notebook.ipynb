{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMm71o3Kxb9O0Biycl8IO73"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83zOVuayp_XQ",
        "outputId": "257d89d1-19b3-4cd2-c67c-ad8a57a2311c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.8/266.8 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.0/297.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.8/211.8 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb --quiet\n",
        "!pip install swig --quiet\n",
        "!pip install gym[all] --quiet\n",
        "!pip install pygame --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "uyhcMZqeR8h2"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## fixing the pygame error\n",
        "\n",
        "import os\n",
        "os.environ['SDL_VIDEODRIVER']='dummy'\n",
        "import pygame\n",
        "pygame.display.set_mode((640,480))"
      ],
      "metadata": {
        "id": "DfgcDdKMGvnH",
        "outputId": "7ce0f9b4-1d17-4509-b18c-c67732f92598",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Surface(640x480x32 SW)>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "from gym.envs import box2d\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "InpayGnesaZ9"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Policy_NN(nn.Module):\n",
        "    def __init__(self, input_size, output_size, hidden_size = 128, gamma = 0.99, learning_rate = 0.001):\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.gamma = gamma\n",
        "        self.learning_rate = learning_rate\n",
        "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
        "        self.layer2 = nn.Linear(hidden_size, output_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "        self.optimizer = optim.Adam(self.parameters(),lr=self.learning_rate)\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "      x = self.layer1(x) #forward propagation\n",
        "      x = self.layer2(x)\n",
        "      x = self.softmax(x)\n",
        "      return x\n",
        "\n",
        "    def act(self,state):\n",
        "        #With obs, find the maximum Q value and output the corresponding action\n",
        "        #action_probs = torch.softmax(self.actor(obs), dim=-1)\n",
        "        #state_value = self.critic(state)\n",
        "\n",
        "        #obs_tensor = torch.as_tensor(obs, dtype=torch.float32)\n",
        "\n",
        "        #action = log_value.detach().item() #action corresponding to max Q index\n",
        "        #return action, log_value\n",
        "        #print(\"checkpoint 6\")\n",
        "        action_probs = torch.softmax(self.policy_network(state), dim=-1)\n",
        "        # q_value = self(obs_tensor.unsqueeze(0)) #convert to row vector\n",
        "        # highest_q_value = torch.argmax(input=q_value)\n",
        "        # log_value = torch.log(q_value.squeeze(0)[highest_q_value])\n",
        "        #print(\"checkpoint 7\")\n",
        "        #print(\"action_probs: {}\".format(action_probs))\n",
        "        #state_value = self.critic(state)\n",
        "        return action_probs"
      ],
      "metadata": {
        "id": "928wfC5eqOZf"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class reinforce_update:\n",
        "    def __init__(self, Policy_NN, rewards, log_value):\n",
        "        self.network = Policy_NN\n",
        "        self.rewards = rewards\n",
        "        self.log_value = log_probs_array\n",
        "\n",
        "    def update_function(self, Policy_NN, rewards, log_probs_array):\n",
        "        discount_rewards = []\n",
        "        Gt,pw = 0, 0\n",
        "        for r in reversed(rewards):\n",
        "            Gt = Gt + gamma**pw * r\n",
        "            pw = pw + 1\n",
        "            discount_rewards.append(Gt)\n",
        "\n",
        "        steps_value = np.arange(rewards.size)\n",
        "\n",
        "        discount_rewards = torch.tensor(discount_rewards)\n",
        "        discount_rewards = discount_rewards[::-1].cumsum()[::-1] / gamma**steps_value\n",
        "\n",
        "        # def discount_rewards(rewards, gamma):\n",
        "        # t_steps = np.arange(rewards.size)\n",
        "        # r = rewards * gamma**t_steps\n",
        "        # r = r[::-1].cumsum()[::-1] / gamma**t_steps\n",
        "        # return r\n",
        "\n",
        "        policy_gradient = []\n",
        "        for log_prob, e_d_r in zip(self.log_value, discount_rewards):\n",
        "            policy_gradient.append(-log_prob*e_d_r)\n",
        "\n",
        "        policy_network.optimizer.zero_grad()\n",
        "        policy_gradient = torch.stack(policy_gradient).sum()\n",
        "        policy_gradient.backward()\n",
        "        policy_network.optimizer.step()\n",
        "\n",
        "        return policy_gradient"
      ],
      "metadata": {
        "id": "ivOk1glCqWtF"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update(policy_network, gamma = 0.99):\n",
        "    log_probs = []\n",
        "    rewards = []"
      ],
      "metadata": {
        "id": "YPnNI28AqdfA"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import gym\n",
        "import statistics"
      ],
      "metadata": {
        "id": "yKKnEL4UqmpZ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    env = gym.make(\"LunarLander-v2\")\n",
        "    state_size = env.observation_space.shape[0]\n",
        "    action_size = env.action_space.n\n",
        "    policy_network = Policy_NN(state_size, action_size)\n",
        "\n",
        "    episode_array, rewards_array = [] , []\n",
        "    n_episode = 500\n",
        "    n_timesteps = 1000\n",
        "\n",
        "    Reward_list = np.empty(shape=n_episode)\n",
        "\n",
        "    state = env.reset()\n",
        "\n",
        "    for episode in range(n_episode):\n",
        "        log_probs_array,rewards = [], []\n",
        "        total_reward = 0\n",
        "        for step in range(n_timesteps):\n",
        "            stateT = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
        "            action_probs = policy_network(stateT)\n",
        "            action_dist = torch.distributions.Categorical(action_probs)\n",
        "            action = action_dist.sample()\n",
        "            log_prob = action_dist.log_prob(action.unsqueeze(0))\n",
        "            log_probs_array.append(log_prob)\n",
        "            next_state, reward, done, _ = env.step(action.item())\n",
        "            print(\"Episode:{} |  Step: {} | action: {} | Reward: {}\".format(episode, step,action.item(),reward))\n",
        "            if done:\n",
        "                reinforce_update.update_function(policy_network, rewards,log_probs_array)\n",
        "                rewards_array.append(np.sum(rewards))\n",
        "                s = env.reset()\n",
        "            else:\n",
        "                s = next_state\n",
        "            # total_reward += reward\n",
        "            # rewards.append(reward)\n",
        "            # state = next_state\n",
        "\n",
        "    discounted_rewards = []\n",
        "    R = 0\n",
        "    for reward in rewards:\n",
        "        R = reward + policy_network.gamma * R\n",
        "        discounted_rewards.insert(0,R)\n",
        "\n",
        "    # Compute policy loss\n",
        "    policy_loss = torch.stack(log_probs) * discounted_rewards\n",
        "    policy_loss = -policy_loss.sum()\n",
        "\n",
        "    # Update policy\n",
        "    optimizer.zero_grad()\n",
        "    policy_loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "id": "QfusON6SqqY1"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SB6gzLAwrnMH",
        "outputId": "5df90f50-6f01-4d95-c6a9-aaf24510371a"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode:0 |  Step: 0 | action: 2 | Reward: -0.6458350116281395\n",
            "Episode:0 |  Step: 1 | action: 3 | Reward: -0.033110083328465406\n",
            "Episode:0 |  Step: 2 | action: 3 | Reward: 0.3416970434502684\n",
            "Episode:0 |  Step: 3 | action: 0 | Reward: -0.6928765795870788\n",
            "Episode:0 |  Step: 4 | action: 1 | Reward: -1.6510315778688198\n",
            "Episode:0 |  Step: 5 | action: 2 | Reward: 0.8631422291630713\n",
            "Episode:0 |  Step: 6 | action: 1 | Reward: -1.8515133665497092\n",
            "Episode:0 |  Step: 7 | action: 1 | Reward: -1.9822982254573265\n",
            "Episode:0 |  Step: 8 | action: 2 | Reward: 1.8720730106944472\n",
            "Episode:0 |  Step: 9 | action: 2 | Reward: 0.2260442478981986\n",
            "Episode:0 |  Step: 10 | action: 1 | Reward: -2.3574167572749887\n",
            "Episode:0 |  Step: 11 | action: 1 | Reward: -2.4061727318996007\n",
            "Episode:0 |  Step: 12 | action: 1 | Reward: -2.617508156238982\n",
            "Episode:0 |  Step: 13 | action: 2 | Reward: 0.02219911625037413\n",
            "Episode:0 |  Step: 14 | action: 2 | Reward: -0.07159662157777119\n",
            "Episode:0 |  Step: 15 | action: 2 | Reward: -0.6710166802392792\n",
            "Episode:0 |  Step: 16 | action: 0 | Reward: -1.931864662783937\n",
            "Episode:0 |  Step: 17 | action: 0 | Reward: -1.9461297504467439\n",
            "Episode:0 |  Step: 18 | action: 2 | Reward: -2.238909509611335\n",
            "Episode:0 |  Step: 19 | action: 2 | Reward: -0.8460837250554392\n",
            "Episode:0 |  Step: 20 | action: 3 | Reward: -0.9110817339345931\n",
            "Episode:0 |  Step: 21 | action: 1 | Reward: -2.774666542576908\n",
            "Episode:0 |  Step: 22 | action: 2 | Reward: -1.0095433201810124\n",
            "Episode:0 |  Step: 23 | action: 3 | Reward: -0.6538813551169642\n",
            "Episode:0 |  Step: 24 | action: 1 | Reward: -2.779217069677229\n",
            "Episode:0 |  Step: 25 | action: 1 | Reward: -3.0722109779308355\n",
            "Episode:0 |  Step: 26 | action: 1 | Reward: -3.307116436462677\n",
            "Episode:0 |  Step: 27 | action: 0 | Reward: -2.3031719815620875\n",
            "Episode:0 |  Step: 28 | action: 2 | Reward: -0.24356521962563987\n",
            "Episode:0 |  Step: 29 | action: 1 | Reward: -3.393029077089038\n",
            "Episode:0 |  Step: 30 | action: 0 | Reward: -2.5571830613813518\n",
            "Episode:0 |  Step: 31 | action: 2 | Reward: -1.35320921250825\n",
            "Episode:0 |  Step: 32 | action: 1 | Reward: -3.8830631172850176\n",
            "Episode:0 |  Step: 33 | action: 2 | Reward: -3.2215042587456137\n",
            "Episode:0 |  Step: 34 | action: 0 | Reward: -2.8322030520428143\n",
            "Episode:0 |  Step: 35 | action: 2 | Reward: -3.8494464372766855\n",
            "Episode:0 |  Step: 36 | action: 1 | Reward: -3.870449636387973\n",
            "Episode:0 |  Step: 37 | action: 1 | Reward: -4.168378107918129\n",
            "Episode:0 |  Step: 38 | action: 1 | Reward: -4.345246336252926\n",
            "Episode:0 |  Step: 39 | action: 2 | Reward: -5.39751336907874\n",
            "Episode:0 |  Step: 40 | action: 1 | Reward: -4.576935876811404\n",
            "Episode:0 |  Step: 41 | action: 3 | Reward: -2.6382700829792882\n",
            "Episode:0 |  Step: 42 | action: 3 | Reward: -2.250092301505076\n",
            "Episode:0 |  Step: 43 | action: 1 | Reward: -4.1372617036312525\n",
            "Episode:0 |  Step: 44 | action: 3 | Reward: -2.2005471745370913\n",
            "Episode:0 |  Step: 45 | action: 1 | Reward: -4.004222302663918\n",
            "Episode:0 |  Step: 46 | action: 3 | Reward: -2.3315180746046438\n",
            "Episode:0 |  Step: 47 | action: 0 | Reward: -3.0722066643072594\n",
            "Episode:0 |  Step: 48 | action: 2 | Reward: -6.121795048658373\n",
            "Episode:0 |  Step: 49 | action: 1 | Reward: -4.0528165511103955\n",
            "Episode:0 |  Step: 50 | action: 2 | Reward: -5.587083556600692\n",
            "Episode:0 |  Step: 51 | action: 2 | Reward: -5.440841185218733\n",
            "Episode:0 |  Step: 52 | action: 3 | Reward: -2.2098471030210747\n",
            "Episode:0 |  Step: 53 | action: 0 | Reward: -3.078475157240632\n",
            "Episode:0 |  Step: 54 | action: 0 | Reward: -3.102332639651877\n",
            "Episode:0 |  Step: 55 | action: 2 | Reward: -5.021583139847837\n",
            "Episode:0 |  Step: 56 | action: 1 | Reward: -4.067003390593157\n",
            "Episode:0 |  Step: 57 | action: 2 | Reward: -7.965169782619841\n",
            "Episode:0 |  Step: 58 | action: 2 | Reward: -9.382453795675463\n",
            "Episode:0 |  Step: 59 | action: 1 | Reward: -4.556560557515382\n",
            "Episode:0 |  Step: 60 | action: 0 | Reward: -3.83870669551294\n",
            "Episode:0 |  Step: 61 | action: 3 | Reward: -2.7806585032233486\n",
            "Episode:0 |  Step: 62 | action: 0 | Reward: -3.6841042649436417\n",
            "Episode:0 |  Step: 63 | action: 1 | Reward: -4.56375431219672\n",
            "Episode:0 |  Step: 64 | action: 1 | Reward: -5.028549663641541\n",
            "Episode:0 |  Step: 65 | action: 1 | Reward: -5.246761924809419\n",
            "Episode:0 |  Step: 66 | action: 3 | Reward: -4.055298166394976\n",
            "Episode:0 |  Step: 67 | action: 2 | Reward: -8.726938654040044\n",
            "Episode:0 |  Step: 68 | action: 1 | Reward: -5.679713538746198\n",
            "Episode:0 |  Step: 69 | action: 2 | Reward: -8.397131469997522\n",
            "Episode:0 |  Step: 70 | action: 2 | Reward: -7.7490670916371185\n",
            "Episode:0 |  Step: 71 | action: 1 | Reward: -6.2604566893724884\n",
            "Episode:0 |  Step: 72 | action: 1 | Reward: -6.83756547402882\n",
            "Episode:0 |  Step: 73 | action: 2 | Reward: -13.339239356722874\n",
            "Episode:0 |  Step: 74 | action: 3 | Reward: -5.993897987200255\n",
            "Episode:0 |  Step: 75 | action: 3 | Reward: 3.94534638157498\n",
            "Episode:0 |  Step: 76 | action: 2 | Reward: -14.230159583484056\n",
            "Episode:0 |  Step: 77 | action: 2 | Reward: -10.034399131779356\n",
            "Episode:0 |  Step: 78 | action: 0 | Reward: -100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "reinforce_update.update_function() missing 1 required positional argument: 'log_probs_array'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-263240bbee7e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-43-83ae228fdc3e>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Episode:{} |  Step: {} | action: {} | Reward: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0mreinforce_update\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy_network\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlog_probs_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m                 \u001b[0mrewards_array\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: reinforce_update.update_function() missing 1 required positional argument: 'log_probs_array'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = episode_array\n",
        "y = rewards_array\n",
        "#plotting the comparison between the two\n",
        "plt.title(\"Performance of REINFORCE\")\n",
        "plt.plot(x, y, label = \"REINFORCE\")\n",
        "plt.xlabel(\"Episodes\")\n",
        "plt.ylabel(\"Rewards\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3IOMJZnltFH1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "b1e52588-e40c-4a54-d6e0-4cea2ae30e28"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'episode_array' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-01bf80ef42b4>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepisode_array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrewards_array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#plotting the comparison between the two\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Performance of REINFORCE\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"REINFORCE\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'episode_array' is not defined"
          ]
        }
      ]
    }
  ]
}