{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNDqpsSVck059W64thNJ3Ed",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/itismeganrms/rl-a3/blob/main/Code/reinforce_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83zOVuayp_XQ",
        "outputId": "257d89d1-19b3-4cd2-c67c-ad8a57a2311c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.8/266.8 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.0/297.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.8/211.8 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb --quiet\n",
        "!pip install swig --quiet\n",
        "!pip install gym[all] --quiet\n",
        "!pip install pygame --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "uyhcMZqeR8h2"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## fixing the pygame error\n",
        "\n",
        "import os\n",
        "os.environ['SDL_VIDEODRIVER']='dummy'\n",
        "import pygame\n",
        "pygame.display.set_mode((640,480))"
      ],
      "metadata": {
        "id": "DfgcDdKMGvnH",
        "outputId": "7ce0f9b4-1d17-4509-b18c-c67732f92598",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Surface(640x480x32 SW)>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "from gym.envs import box2d\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "InpayGnesaZ9"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Policy_NN(nn.Module):\n",
        "    def __init__(self, input_size, output_size, hidden_size = 128, gamma = 0.99, learning_rate = 0.001):\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.gamma = gamma\n",
        "        self.learning_rate = learning_rate\n",
        "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
        "        self.layer2 = nn.Linear(hidden_size, output_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "        self.optimizer = optim.Adam(self.parameters(),lr=self.learning_rate)\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "      x = self.layer1(x) #forward propagation\n",
        "      x = self.layer2(x)\n",
        "      x = self.softmax(x)\n",
        "      return x\n",
        "\n",
        "    def act(self,state):\n",
        "        #With obs, find the maximum Q value and output the corresponding action\n",
        "        #action_probs = torch.softmax(self.actor(obs), dim=-1)\n",
        "        #state_value = self.critic(state)\n",
        "\n",
        "        #obs_tensor = torch.as_tensor(obs, dtype=torch.float32)\n",
        "        #q_value = self(obs_tensor.unsqueeze(0)) #convert to row vector\n",
        "        #highest_q_value = torch.argmax(input=q_value)\n",
        "        #log_value = torch.log(q_value.squeeze(0)[highest_q_value])\n",
        "        #action = log_value.detach().item() #action corresponding to max Q index\n",
        "        #return action, log_value\n",
        "        #print(\"checkpoint 6\")\n",
        "        action_probs = torch.softmax(self.policy_network(state), dim=-1)\n",
        "        #print(\"checkpoint 7\")\n",
        "        #print(\"action_probs: {}\".format(action_probs))\n",
        "        #state_value = self.critic(state)\n",
        "        return action_probs"
      ],
      "metadata": {
        "id": "928wfC5eqOZf"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class reinforce_update:\n",
        "    def __init__(self, Policy_NN, rewards, log_value):\n",
        "        self.network = Policy_NN\n",
        "        self.rewards = rewards\n",
        "        self.log_value = log_probs_array\n",
        "\n",
        "    def update_function(self, Policy_NN, rewards, log_probs_array):\n",
        "        discount_rewards = []\n",
        "        Gt,pw = 0, 0\n",
        "        for r in reversed(rewards):\n",
        "            Gt = Gt + gamma**pw * r\n",
        "            pw = pw + 1\n",
        "            discount_rewards.append(Gt)\n",
        "\n",
        "        steps_value = np.arange(rewards.size)\n",
        "\n",
        "        discount_rewards = torch.tensor(discount_rewards)\n",
        "        discount_rewards = discount_rewards[::-1].cumsum()[::-1] / gamma**steps_value\n",
        "\n",
        "        # def discount_rewards(rewards, gamma):\n",
        "        # t_steps = np.arange(rewards.size)\n",
        "        # r = rewards * gamma**t_steps\n",
        "        # r = r[::-1].cumsum()[::-1] / gamma**t_steps\n",
        "        # return r\n",
        "\n",
        "        policy_gradient = []\n",
        "        for log_prob, e_d_r in zip(self.log_value, discount_rewards):\n",
        "            policy_gradient.append(-log_prob*e_d_r)\n",
        "\n",
        "        policy_network.optimizer.zero_grad()\n",
        "        policy_gradient = torch.stack(policy_gradient).sum()\n",
        "        policy_gradient.backward()\n",
        "        policy_network.optimizer.step()\n",
        "\n",
        "        return policy_gradient"
      ],
      "metadata": {
        "id": "ivOk1glCqWtF"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update(policy_network, gamma = 0.99):\n",
        "    log_probs = []\n",
        "    rewards = []"
      ],
      "metadata": {
        "id": "YPnNI28AqdfA"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import gym\n",
        "import statistics"
      ],
      "metadata": {
        "id": "yKKnEL4UqmpZ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    env = gym.make(\"LunarLander-v2\")\n",
        "    state_size = env.observation_space.shape[0]\n",
        "    action_size = env.action_space.n\n",
        "    policy_network = Policy_NN(state_size, action_size)\n",
        "\n",
        "    episode_array, rewards_array = [] , []\n",
        "    n_episode = 500\n",
        "    n_timesteps = 1000\n",
        "\n",
        "    Reward_list = np.empty(shape=n_episode)\n",
        "\n",
        "    state = env.reset()\n",
        "\n",
        "    for episode in range(n_episode):\n",
        "        log_probs_array,rewards = [], []\n",
        "        total_reward = 0\n",
        "        for step in range(n_timesteps):\n",
        "            stateT = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
        "            action_probs = policy_network(stateT)\n",
        "            action_dist = torch.distributions.Categorical(action_probs)\n",
        "            action = action_dist.sample()\n",
        "            log_prob = action_dist.log_prob(action.unsqueeze(0))\n",
        "            next_state, reward, done, _ = env.step(action.item())\n",
        "            print(\"Episode:{} |  Step: {} | action: {} | Reward: {}\".format(episode, step,action.item(),reward))\n",
        "            if done:\n",
        "                reinforce_update.update_function(policy_network, rewards,log_probs_array)\n",
        "                rewards_array.append(np.sum(rewards))\n",
        "                s = env.reset()\n",
        "            else:\n",
        "                s = next_state\n",
        "            # total_reward += reward\n",
        "            # rewards.append(reward)\n",
        "            # state = next_state\n",
        "\n",
        "    discounted_rewards = []\n",
        "    R = 0\n",
        "    for reward in rewards:\n",
        "        R = reward + policy_network.gamma * R\n",
        "        discounted_rewards.insert(0,R)\n",
        "\n",
        "    # Compute policy loss\n",
        "    policy_loss = torch.stack(log_probs) * discounted_rewards\n",
        "    policy_loss = -policy_loss.sum()\n",
        "\n",
        "    # Update policy\n",
        "    optimizer.zero_grad()\n",
        "    policy_loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "id": "QfusON6SqqY1"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SB6gzLAwrnMH",
        "outputId": "d08b0fb5-1e35-4089-f0c7-8e10c9f1fc6c"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode:0 |  Step: 0 | action: 3 | Reward: -2.1968751823225134\n",
            "Episode:0 |  Step: 1 | action: 3 | Reward: -2.39859730875571\n",
            "Episode:0 |  Step: 2 | action: 1 | Reward: -1.8057036958187769\n",
            "Episode:0 |  Step: 3 | action: 3 | Reward: -2.2393397397835018\n",
            "Episode:0 |  Step: 4 | action: 2 | Reward: 4.240361241312127\n",
            "Episode:0 |  Step: 5 | action: 0 | Reward: -2.0035647890476582\n",
            "Episode:0 |  Step: 6 | action: 0 | Reward: -1.9464367181026319\n",
            "Episode:0 |  Step: 7 | action: 1 | Reward: -1.6270788406020233\n",
            "Episode:0 |  Step: 8 | action: 2 | Reward: 3.8309755061683974\n",
            "Episode:0 |  Step: 9 | action: 3 | Reward: -2.221772831807441\n",
            "Episode:0 |  Step: 10 | action: 0 | Reward: -1.8547247300237188\n",
            "Episode:0 |  Step: 11 | action: 1 | Reward: -1.5624170571761613\n",
            "Episode:0 |  Step: 12 | action: 2 | Reward: 3.493442737919179\n",
            "Episode:0 |  Step: 13 | action: 1 | Reward: -1.2379095756786274\n",
            "Episode:0 |  Step: 14 | action: 3 | Reward: -1.8140115442231763\n",
            "Episode:0 |  Step: 15 | action: 3 | Reward: -2.0821232280190416\n",
            "Episode:0 |  Step: 16 | action: 1 | Reward: -1.4496462447096679\n",
            "Episode:0 |  Step: 17 | action: 2 | Reward: 2.2330240927276863\n",
            "Episode:0 |  Step: 18 | action: 3 | Reward: -1.8773616293644\n",
            "Episode:0 |  Step: 19 | action: 2 | Reward: 2.5376489077332964\n",
            "Episode:0 |  Step: 20 | action: 1 | Reward: -1.2976908825887665\n",
            "Episode:0 |  Step: 21 | action: 1 | Reward: -1.1098825920337834\n",
            "Episode:0 |  Step: 22 | action: 2 | Reward: 3.6804002375063307\n",
            "Episode:0 |  Step: 23 | action: 2 | Reward: 4.720319425528726\n",
            "Episode:0 |  Step: 24 | action: 3 | Reward: -1.7891728197008934\n",
            "Episode:0 |  Step: 25 | action: 1 | Reward: -0.9814027437226673\n",
            "Episode:0 |  Step: 26 | action: 1 | Reward: -0.6272309733359112\n",
            "Episode:0 |  Step: 27 | action: 1 | Reward: -0.40370650150586473\n",
            "Episode:0 |  Step: 28 | action: 2 | Reward: 4.377284167993804\n",
            "Episode:0 |  Step: 29 | action: 1 | Reward: -0.3135301964628059\n",
            "Episode:0 |  Step: 30 | action: 0 | Reward: -0.37096037257509806\n",
            "Episode:0 |  Step: 31 | action: 2 | Reward: 3.2610230966101996\n",
            "Episode:0 |  Step: 32 | action: 3 | Reward: -0.8476920597683193\n",
            "Episode:0 |  Step: 33 | action: 0 | Reward: -0.5740481118666594\n",
            "Episode:0 |  Step: 34 | action: 2 | Reward: 1.6405083850776634\n",
            "Episode:0 |  Step: 35 | action: 0 | Reward: -0.563694409692431\n",
            "Episode:0 |  Step: 36 | action: 1 | Reward: -0.21557440191105456\n",
            "Episode:0 |  Step: 37 | action: 2 | Reward: 3.9731092120770883\n",
            "Episode:0 |  Step: 38 | action: 3 | Reward: -1.2350192960940365\n",
            "Episode:0 |  Step: 39 | action: 1 | Reward: -1.109693433306801\n",
            "Episode:0 |  Step: 40 | action: 0 | Reward: -1.1602831073957134\n",
            "Episode:0 |  Step: 41 | action: 0 | Reward: -1.1014625175039328\n",
            "Episode:0 |  Step: 42 | action: 1 | Reward: -1.157467545748516\n",
            "Episode:0 |  Step: 43 | action: 0 | Reward: -1.1740647554782413\n",
            "Episode:0 |  Step: 44 | action: 3 | Reward: -0.9884158417246158\n",
            "Episode:0 |  Step: 45 | action: 1 | Reward: -1.0546071565645707\n",
            "Episode:0 |  Step: 46 | action: 3 | Reward: -0.941812009446976\n",
            "Episode:0 |  Step: 47 | action: 0 | Reward: -0.8533344989110958\n",
            "Episode:0 |  Step: 48 | action: 1 | Reward: -0.9893466561136381\n",
            "Episode:0 |  Step: 49 | action: 0 | Reward: -0.9105620742327005\n",
            "Episode:0 |  Step: 50 | action: 2 | Reward: 3.4030172425812397\n",
            "Episode:0 |  Step: 51 | action: 3 | Reward: -0.6532898638037057\n",
            "Episode:0 |  Step: 52 | action: 1 | Reward: -0.7873533729643032\n",
            "Episode:0 |  Step: 53 | action: 2 | Reward: 3.8450180286800846\n",
            "Episode:0 |  Step: 54 | action: 0 | Reward: -0.7882083891251455\n",
            "Episode:0 |  Step: 55 | action: 0 | Reward: -0.7304236102651487\n",
            "Episode:0 |  Step: 56 | action: 1 | Reward: -0.9691223641559918\n",
            "Episode:0 |  Step: 57 | action: 0 | Reward: -0.8147078866534514\n",
            "Episode:0 |  Step: 58 | action: 1 | Reward: -1.0983288148653674\n",
            "Episode:0 |  Step: 59 | action: 1 | Reward: -1.1026202197427597\n",
            "Episode:0 |  Step: 60 | action: 0 | Reward: -1.0155572823100272\n",
            "Episode:0 |  Step: 61 | action: 1 | Reward: -1.1872716708363658\n",
            "Episode:0 |  Step: 62 | action: 2 | Reward: 4.677256046297129\n",
            "Episode:0 |  Step: 63 | action: 0 | Reward: -1.2375652823737369\n",
            "Episode:0 |  Step: 64 | action: 3 | Reward: -1.0001886202366836\n",
            "Episode:0 |  Step: 65 | action: 3 | Reward: -0.742998809212678\n",
            "Episode:0 |  Step: 66 | action: 0 | Reward: -0.8271254480216044\n",
            "Episode:0 |  Step: 67 | action: 1 | Reward: -1.2700905021824542\n",
            "Episode:0 |  Step: 68 | action: 1 | Reward: -1.4969949055844143\n",
            "Episode:0 |  Step: 69 | action: 1 | Reward: -1.9240320777851412\n",
            "Episode:0 |  Step: 70 | action: 3 | Reward: -1.668773996050901\n",
            "Episode:0 |  Step: 71 | action: 0 | Reward: 7.10378097635359\n",
            "Episode:0 |  Step: 72 | action: 1 | Reward: -100\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "reinforce_update.update_function() missing 1 required positional argument: 'log_probs_array'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-263240bbee7e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-32-258f816da07a>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Episode:{} |  Step: {} | action: {} | Reward: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0mreinforce_update\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy_network\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlog_probs_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m                 \u001b[0mrewards_array\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: reinforce_update.update_function() missing 1 required positional argument: 'log_probs_array'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = episode_array\n",
        "y = rewards_array\n",
        "#plotting the comparison between the two\n",
        "plt.title(\"Performance of REINFORCE\")\n",
        "plt.plot(x, y, label = \"REINFORCE\")\n",
        "plt.xlabel(\"Episodes\")\n",
        "plt.ylabel(\"Rewards\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3IOMJZnltFH1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "92b968ae-7894-488c-cb7b-824c1c725ee4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'episode_array' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-01bf80ef42b4>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepisode_array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrewards_array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#plotting the comparison between the two\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Performance of REINFORCE\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"REINFORCE\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'episode_array' is not defined"
          ]
        }
      ]
    }
  ]
}