# -*- coding: utf-8 -*-
"""reinforce.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GlbkyYZpN8gSi0_aDMgcyTJ_Rh93BkXl
"""

!pip install wandb --quiet
!pip install swig --quiet
!pip install gym[all] --quiet
!pip install pygame --quiet

#import nn

class Actor(nn.Module):
    def __init__(self, input_size, output_size, hidden_size = 128, gamma = 0.99, learning_rate = 0.001):
        super().__init__()
        self.input_size = input_size
        self.output_size = output_size
        self.hidden_size = hidden_size
        self.gamma = gamma
        self.learning_rate = learning_rate
        self.layer1 = nn.Linear(input_size, hidden_size)
        self.layer2 = nn.Linear(hidden_size, output_size)
        self.relu = nn.ReLU()
        self.softmax = nn.Softmax(dim=-1)

        self.optimizer = optim.Adam(self.parameters(),lr=self.learning_rate)


    def forward(self,x):
      x = self.layer1(x) #forward propagation
      x = self.layer2(x)
      x = self.softmax(x)
      return x

    def act(self,state):
        #With obs, find the maximum Q value and output the corresponding action
        #action_probs = torch.softmax(self.actor(obs), dim=-1)
        #state_value = self.critic(state)

        #obs_tensor = torch.as_tensor(obs, dtype=torch.float32)
        #q_value = self(obs_tensor.unsqueeze(0)) #convert to row vector
        #highest_q_value = torch.argmax(input=q_value)
        #log_value = torch.log(q_value.squeeze(0)[highest_q_value])
        #action = log_value.detach().item() #action corresponding to max Q index
        #return action, log_value
        #print("checkpoint 6")
        action_probs = torch.softmax(self.actor(state), dim=-1)
        #print("checkpoint 7")
        #print("action_probs: {}".format(action_probs))
        #state_value = self.critic(state)
        return action_probs

class Reinforce_Update:
    def __init__(self, Policy_NN, rewards, log_value):
        self.network = Policy_NN
        self.rewards = rewards
        self.log_value = log_value

    def update_function(self, gamma, pw):
        discount_rewards = []
        Gt,pw = 0, 0
        for r in reversed(rewards):
            Gt = Gt + gamma**pw * r
            pw = pw + 1
            discount_rewards.append(Gt)

        steps_value = np.arange(rewards.size)

        discount_rewards = torch.tensor(discount_rewards)
        discount_rewards = discount_rewards[::-1].cumsum()[::-1] / gamma**steps_value

        # def discount_rewards(rewards, gamma):
        # t_steps = np.arange(rewards.size)
        # r = rewards * gamma**t_steps
        # r = r[::-1].cumsum()[::-1] / gamma**t_steps
        # return r

        policy_gradient = []
        for log_prob, e_d_r in zip(self.log_value, discount_rewards):
            policy_gradient.append(-log_prob*e_d_r)

        policy_network.optimizer.zero_grad()
        policy_gradient = torch.stack(policy_gradient).sum()
        policy_gradient.backward()
        policy_network.optimizer.step()

def update(actor, gamma = 0.99):
  log_probs = []
  rewards = []

import torch
import numpy as np
import gym
import statistics


def main():
  env = gym.make("LunarLander-v2")
  state_size = env.observation_space.shape[0]
  action_size = env.action_space.n
  actor = Actor(state_size, action_size)


  print("checkpoint 1")

  episode_array, rewards_array = [] , []

  n_episode = 1000
  n_timesteps = 1000

  print("checkpoint 2")

  Reward_list = np.empty(shape=n_episode)
  print("checkpoint 3")

  state = env.reset()

  print("checkpoint 4")

  for episode in range(n_episode):

    log_probs,rewards = [], []
    total_reward = 0
    for step in range(n_timesteps):

        stateT = torch.tensor(state, dtype=torch.float32).unsqueeze(0)

        action_probs = actor(stateT)
        action_dist = torch.distributions.Categorical(action_probs)
        action = action_dist.sample()
        #action = action.item()
        #action =
        #actionT = torch.tensor(action)

        log_prob = action_dist.log_prob(action.unsqueeze(0))

        #log_prob = action_dist.log_prob(action)
        #action = torch.argmax(action_probs).item()
        print("action: {}".format(action.item()))
        next_state, reward, done, _ = env.step(action.item())
        total_reward += reward
        rewards.append(reward)
        state = next_state

    discounted_rewards = []
    R = 0

    for reward in rewards[::1]:
      R = reward + actor.gamma * R
      discounted_rewards.insert(0,R)

    #discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-8)

    # Compute policy loss
    policy_loss = torch.stack(log_probs) * discounted_rewards
    policy_loss = -policy_loss.sum()

    # Update policy
    optimizer.zero_grad()
    policy_loss.backward()
    optimizer.step()




main()