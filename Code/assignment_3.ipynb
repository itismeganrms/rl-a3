{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb --quiet\n",
        "!pip install swig --quiet\n",
        "!pip install gym[all] --quiet\n",
        "!pip install pygame --quiet"
      ],
      "metadata": {
        "id": "4XnhpfQtAOYN",
        "collapsed": true
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "uyhcMZqeR8h2"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## fixing the pygame error\n",
        "\n",
        "import os\n",
        "os.environ['SDL_VIDEODRIVER']='dummy'\n",
        "import pygame\n",
        "pygame.display.set_mode((640,480))"
      ],
      "metadata": {
        "id": "DfgcDdKMGvnH",
        "outputId": "03200d7c-adc4-4788-f0f2-089cbcc74b81",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Surface(640x480x32 SW)>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "from gym.envs import box2d\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "TjJi6_jkBwI3"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Policy_NN(nn.Module):\n",
        "    def __init__(self,observation_space,action_space,gamma,learning_rate):\n",
        "        super().__init__()\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_features= observation_space, out_features = 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features= 128, out_features = action_space))\n",
        "\n",
        "        self.observation_space = observation_space\n",
        "        self.action_space = action_space\n",
        "        self.gamma = gamma\n",
        "        self.learning_rate = learning_rate\n",
        "        self.optimizer = optim.Adam(self.parameters(),lr=self.learning_rate)\n",
        "\n",
        "    def forward(self,x):\n",
        "      return self.net(x) #forward propagation\n",
        "\n",
        "    def act(self,obs):\n",
        "        #With obs, find the maximum Q value and output the corresponding action\n",
        "        obs_tensor = torch.as_tensor(obs, dtype=torch.float32)\n",
        "        q_value = self(obs_tensor.unsqueeze(0)) #convert to row vector\n",
        "        highest_q_value = torch.argmax(input=q_value)\n",
        "        log_value = torch.log(q_value.squeeze(0)[highest_q_value])\n",
        "        action = abs(int(log_value.detach().item())) #action corresponding to max Q index\n",
        "        return action, log_value"
      ],
      "metadata": {
        "id": "aGard6TWVpm7"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class reinforce_update:\n",
        "    def __init__(self, Policy_NN, rewards, log_value):\n",
        "        self.network = Policy_NN\n",
        "        self.rewards = rewards\n",
        "        self.log_value = log_value\n",
        "\n",
        "    def update_function(self):\n",
        "        discount_rewards = []\n",
        "        Gt,pw = 0, 0\n",
        "        for r in reversed(rewards):\n",
        "            Gt = Gt + gamma**pw * r\n",
        "            pw = pw + 1\n",
        "            discount_rewards.append(Gt)\n",
        "\n",
        "        steps_value = np.arange(rewards.size)\n",
        "\n",
        "        discount_rewards = torch.tensor(discount_rewards)\n",
        "        discount_rewards = discount_rewards[::-1].cumsum()[::-1] / gamma**steps_value\n",
        "\n",
        "        # def discount_rewards(rewards, gamma):\n",
        "        # t_steps = np.arange(rewards.size)\n",
        "        # r = rewards * gamma**t_steps\n",
        "        # r = r[::-1].cumsum()[::-1] / gamma**t_steps\n",
        "        # return r\n",
        "\n",
        "        policy_gradient = []\n",
        "        for log_prob, e_d_r in zip(self.log_value, discount_rewards):\n",
        "            policy_gradient.append(-log_prob*e_d_r)\n",
        "\n",
        "        policy_network.optimizer.zero_grad()\n",
        "        policy_gradient = torch.stack(policy_gradient).sum()\n",
        "        policy_gradient.backward()\n",
        "        policy_network.optimizer.step()"
      ],
      "metadata": {
        "id": "XwBk5pZPRUJ_"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gamma = 0.97\n",
        "learning_rate = 0.1"
      ],
      "metadata": {
        "id": "t9XavTFgPF8_"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"LunarLander-v2\", render_mode=\"human\")\n",
        "observation_space = env.observation_space.shape[0]\n",
        "action_space = env.action_space.n\n",
        "policy_network = Policy_NN(observation_space,action_space,gamma,learning_rate)\n",
        "\n",
        "episode_array, rewards_array = [] , []\n",
        "\n",
        "n_episode = 1000\n",
        "n_timesteps = 1000\n",
        "\n",
        "\n",
        "Reward_list = np.empty(shape=n_episode)\n",
        "\n",
        "s = env.reset()\n",
        "\n",
        "for episode in range(n_episode):\n",
        "\n",
        "    log_probs,rewards = [], []\n",
        "    for step in range(n_timesteps):\n",
        "        action,log_prob = policy_network.act(s)\n",
        "        s_next, r, done, _ = env.step(action)\n",
        "        log_probs.append(log_prob)\n",
        "        rewards.append(r)\n",
        "        if done:\n",
        "            reinforce_update.update_function(policy_network, rewards,log_probs)\n",
        "            rewards_array.append(np.sum(rewards))\n",
        "            s = env.reset()\n",
        "        else:\n",
        "            s = s_next"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "5noi-PwdThNT",
        "outputId": "4b62ae05-2f6d-426a-e790-025512c71fa5"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "reinforce_update.update_function() takes 1 positional argument but 3 were given",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-884f981c9d35>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mrewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0mreinforce_update\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy_network\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlog_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0mrewards_array\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: reinforce_update.update_function() takes 1 positional argument but 3 were given"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = episode_array\n",
        "y = rewards_array\n",
        "#plotting the comparison between the two\n",
        "plt.title(\"Performance of REINFORCE\")\n",
        "plt.plot(x, y, label = \"REINFORCE\")\n",
        "plt.xlabel(\"Episodes\")\n",
        "plt.ylabel(\"Rewards\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3Xoqdn6kTmmw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adding WandB section"
      ],
      "metadata": {
        "id": "FvBmos00Sv9W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb login"
      ],
      "metadata": {
        "id": "v3Z9VnRwADPv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZtjpzS2Qu1O1"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# start a new wandb run to track this script\n",
        "wandb.init(\n",
        "    # set the wandb project where this run will be logged\n",
        "    project=\"my-awesome-project\",\n",
        "\n",
        "    # track hyperparameters and run metadata\n",
        "    config={\n",
        "    \"learning_rate\": 0.02,\n",
        "    \"architecture\": \"CNN\",\n",
        "    \"dataset\": \"CIFAR-100\",\n",
        "    \"epochs\": 10,\n",
        "    }\n",
        ")\n",
        "\n",
        "# simulate training\n",
        "epochs = 10\n",
        "offset = random.random() / 5\n",
        "for epoch in range(2, epochs):\n",
        "    acc = 1 - 2 ** -epoch - random.random() / epoch - offset\n",
        "    loss = 2 ** -epoch + random.random() / epoch + offset\n",
        "\n",
        "    # log metrics to wandb\n",
        "    wandb.log({\"acc\": acc, \"loss\": loss})\n",
        "\n",
        "# [optional] finish the wandb run, necessary in notebooks\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "9j1u20RrAQ_A"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}